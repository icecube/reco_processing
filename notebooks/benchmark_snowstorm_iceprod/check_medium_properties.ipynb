{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import simweights\n",
    "import pickle\n",
    "import os, sys\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "from matplotlib.gridspec import GridSpec\n",
    "from matplotlib.lines import Line2D\n",
    "\n",
    "import matplotlib as mat\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import matplotlib.colors as colors\n",
    "import pandas as pd\n",
    "import tables\n",
    "import h5py\n",
    "import math\n",
    "from scipy.stats import mstats\n",
    "import matplotlib as mpl\n",
    "import matplotlib.font_manager as font_manager\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(\"/data/user/tvaneede/GlobalFit/reco_processing/notebooks/weighting\")\n",
    "from weights import *\n",
    "from utils import *\n",
    "from selections import selection_mask\n",
    "from fonts import *\n",
    "from plot_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Append the custom module path\n",
    "sys.path.append(\"/data/user/tvaneede/GlobalFit/reco_processing\")\n",
    "\n",
    "# Import the datasets module\n",
    "from datasets import datasets_snowstorm_iceprod_benchmark as datasets\n",
    "\n",
    "# set the inputs\n",
    "reco_versions =  [\"snowstorm_iceprod_benchmark_level2\"]\n",
    "\n",
    "# Dynamically select the desired dataset\n",
    "simulation_datasets = {}\n",
    "for reco_version in reco_versions: simulation_datasets[reco_version] = getattr(datasets, reco_version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "livetime_yr = 11.687\n",
    "livetime_s  = livetime_yr * 365.25 * 24 * 3600 # 11.687 year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plotting_path = f\"/data/user/tvaneede/GlobalFit/reco_processing/notebooks/benchmark_snowstorm_iceprod/check_medium_properties\"\n",
    "os.system(f\"mkdir -p {plotting_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weight functions\n",
    "gamma_astro = 2.87\n",
    "per_flavor_norm = 2.12\n",
    "AstroFluxModel_HESE = create_AstroFluxModel(per_flavor_norm=per_flavor_norm, gamma_astro=gamma_astro)\n",
    "\n",
    "gamma_astro = 2.53\n",
    "per_flavor_norm = 1.66\n",
    "AstroFluxModel_cascade = create_AstroFluxModel(per_flavor_norm=per_flavor_norm, gamma_astro=gamma_astro)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_datasets( simulation_dataset, keys_to_merge ):\n",
    "\n",
    "    # open the files\n",
    "    for key in simulation_dataset:\n",
    "        print(f\"----- Extracting files for {key}\")\n",
    "        simulation_dataset[key]['hdf_file'] = pd.HDFStore(simulation_dataset[key]['hdf_file_path'],'r')\n",
    "        simulation_dataset[key]['weighter'] = simweights.NuGenWeighter( simulation_dataset[key]['hdf_file'] ,nfiles=simulation_dataset[key]['nfiles'])\n",
    "\n",
    "    # merging files\n",
    "    for new_key in keys_to_merge:\n",
    "        print(f\"----- Creating new key {new_key}\")\n",
    "        simulation_dataset[new_key] = {}\n",
    "        simulation_dataset[new_key]['variables'] = {}\n",
    "        simulation_dataset[new_key]['weighter'] = None\n",
    "\n",
    "        for key in keys_to_merge[new_key]:\n",
    "            \n",
    "            print(f\"Using {key}\")\n",
    "            # merge the weighters\n",
    "            if simulation_dataset[new_key]['weighter'] == None:\n",
    "                simulation_dataset[new_key]['weighter'] = simulation_dataset[key]['weighter']\n",
    "            else: simulation_dataset[new_key]['weighter'] += simulation_dataset[key]['weighter']\n",
    "\n",
    "    return simulation_dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys_to_merge = {}\n",
    "\n",
    "\n",
    "\n",
    "keys_to_merge[\"snowstorm_iceprod_benchmark_level2\"] = {\n",
    "    \"NuE\" : [\"NuE_midE\", \"NuE_highE\"],\n",
    "    \"NuMu\" : [\"NuMu_midE\", \"NuMu_highE\"],\n",
    "    \"NuTau\" : [\"NuTau_midE\", \"NuTau_highE\"],\n",
    "    \"NuAll\" : ['NuE', \"NuMu\", \"NuTau\"],\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- Extracting files for NuTau_lowE\n",
      "----- Extracting files for NuTau_midE\n",
      "----- Extracting files for NuTau_highE\n",
      "----- Extracting files for NuE_lowE\n",
      "----- Extracting files for NuE_midE\n",
      "----- Extracting files for NuE_highE\n",
      "----- Extracting files for NuMu_lowlowE\n",
      "----- Extracting files for NuMu_lowE\n",
      "----- Extracting files for NuMu_midE\n",
      "----- Extracting files for NuMu_highE\n",
      "----- Creating new key NuE\n",
      "Using NuE_midE\n",
      "Using NuE_highE\n",
      "----- Creating new key NuMu\n",
      "Using NuMu_midE\n",
      "Using NuMu_highE\n",
      "----- Creating new key NuTau\n",
      "Using NuTau_midE\n",
      "Using NuTau_highE\n",
      "----- Creating new key NuAll\n",
      "Using NuE\n",
      "Using NuMu\n",
      "Using NuTau\n"
     ]
    }
   ],
   "source": [
    "for key in simulation_datasets: simulation_datasets[key] = open_datasets( simulation_datasets[key], keys_to_merge[key] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_label_dict = {\n",
    "    \"snowstorm_iceprod_benchmark_level2\" : \"level2 ensemble\",\n",
    "}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3-v4.4.1_reco-v1.1.0",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
