{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import matplotlib.pyplot as plt\n",
    "import h5py\n",
    "import pandas as pd\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets_snowstorm = {\n",
    "  23460: {\"flavor\": \"NuTau\", \"energy\": \"high\", \"nfiles\" : 22000},\n",
    "  23459: {\"flavor\": \"NuTau\", \"energy\": \"mid\", \"nfiles\" : 4000},\n",
    "  23458: {\"flavor\": \"NuTau\", \"energy\": \"low\", \"nfiles\" : 1100},\n",
    "\n",
    "  23457: {\"flavor\": \"NuE\", \"energy\": \"high\", \"nfiles\" : 24000},\n",
    "  23456: {\"flavor\": \"NuE\", \"energy\": \"mid\", \"nfiles\" : 4200},\n",
    "  23455: {\"flavor\": \"NuE\", \"energy\": \"low\", \"nfiles\" : 900},\n",
    "\n",
    "  23454: {\"flavor\": \"NuMu\", \"energy\": \"high\", \"nfiles\" : 18000},\n",
    "  23453: {\"flavor\": \"NuMu\", \"energy\": \"mid\", \"nfiles\" : 5400},\n",
    "  23452: {\"flavor\": \"NuMu\", \"energy\": \"low\", \"nfiles\" : 8100},\n",
    "  23451: {\"flavor\": \"NuMu\", \"energy\": \"lowlow\", \"nfiles\" : 4000},\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open datasets\n",
    "for dataset_id in datasets_snowstorm:\n",
    "    datasets_snowstorm[dataset_id][\"df\"] = df = pd.read_hdf(f\"/data/user/tvaneede/GlobalFit/reco_processing/notebooks/benchmark_snowstorm_iceprod/requirements/data/{dataset_id}.hdf5\", key=f'/{dataset_id}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_dict = {\n",
    "    \"high\" : {\n",
    "        0  : \"NuGen+CORSIKA+Polyplopia+MuonProp\",\n",
    "        1  : \"PhotonProp\",\n",
    "        2  : \"Detector+L1+L2\",\n",
    "        3  : \"Filter_HESE+Taupede\",\n",
    "        4  : \"EvtGen_HESE\",\n",
    "        5  : \"FinalLevel_DiffuseNuMu\",\n",
    "        6  : \"Level3_Cascade\",\n",
    "        7  : \"Level4_Cascade\",\n",
    "        8  : \"Level5_Cascade_cascade\",\n",
    "        9  : \"Level5_Cascade_muon\",\n",
    "        10 : \"Level5_Cascade_hybrid\",\n",
    "        11 : \"Level6_Cascade_cascade\",\n",
    "        12 : \"Level6_Cascade_muon\",\n",
    "        13 : \"Level6_Cascade_hybrid\",\n",
    "        14 : \"Level7_Cascade_cascade\",\n",
    "        15 : \"Level8_Cascade_cascade\",\n",
    "    },\n",
    "    \"mid\" : {\n",
    "        0  : \"NuGen+CORSIKA+Polyplopia+MuonProp\",\n",
    "        1  : \"PhotonProp\",\n",
    "        2  : \"Detector+L1+L2\",\n",
    "        3  : \"Filter_HESE+Taupede\",\n",
    "        4  : \"EvtGen_HESE\",\n",
    "        5  : \"FinalLevel_DiffuseNuMu\",\n",
    "        6  : \"Level3_Cascade\",\n",
    "        7  : \"Level4_Cascade\",\n",
    "        8  : \"Level5_Cascade_cascade\",\n",
    "        9  : \"Level5_Cascade_muon\",\n",
    "        10 : \"Level5_Cascade_hybrid\",\n",
    "        11 : \"Level6_Cascade_cascade\",\n",
    "        12 : \"Level6_Cascade_muon\",\n",
    "        13 : \"Level6_Cascade_hybrid\",\n",
    "        14 : \"Level7_Cascade_cascade\",\n",
    "        15 : \"Level8_Cascade_cascade\",\n",
    "    },\n",
    "    \"low\" : {\n",
    "        0  : \"NuGen+CORSIKA+Polyplopia+MuonProp\",\n",
    "        1  : \"PhotonProp\",\n",
    "        2  : \"Detector+L1+L2\",\n",
    "        5  : \"FinalLevel_DiffuseNuMu\",\n",
    "        6  : \"Level3_Cascade\",\n",
    "        7  : \"Level4_Cascade\",\n",
    "        8  : \"Level5_Cascade_cascade\",\n",
    "        9  : \"Level5_Cascade_muon\",\n",
    "        10 : \"Level5_Cascade_hybrid\",\n",
    "        11 : \"Level6_Cascade_cascade\",\n",
    "        12 : \"Level6_Cascade_muon\",\n",
    "        13 : \"Level6_Cascade_hybrid\",\n",
    "    },\n",
    "    \"lowlow\" : {\n",
    "        0  : \"NuGen+CORSIKA+Polyplopia+MuonProp\",\n",
    "        1  : \"PhotonProp\",\n",
    "        2  : \"Detector+L1+L2\",\n",
    "        5  : \"FinalLevel_DiffuseNuMu\",\n",
    "        6  : \"Level3_Cascade\",\n",
    "        7  : \"Level4_Cascade\",\n",
    "        8  : \"Level5_Cascade_cascade\",\n",
    "        9  : \"Level5_Cascade_muon\",\n",
    "        10 : \"Level5_Cascade_hybrid\",\n",
    "        11 : \"Level6_Cascade_cascade\",\n",
    "        12 : \"Level6_Cascade_muon\",\n",
    "        13 : \"Level6_Cascade_hybrid\",\n",
    "    },\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_mean_usage( variable, dataset_id, df, energy ):\n",
    "\n",
    "    # Find the number of tasks in this dataset\n",
    "    tasks = df.index.get_level_values('task').unique()\n",
    "    tasks = tasks[tasks >= 0]  # just in case\n",
    "\n",
    "    result = {\"total\" : 0}\n",
    "\n",
    "    for i, task in enumerate(tasks):\n",
    "        # Mask for this dataset and task\n",
    "        mask = (df.index.get_level_values('dataset') == dataset_id) & \\\n",
    "            (df.index.get_level_values('task') == task)\n",
    "        df_mask = df[mask]\n",
    "        mean = df_mask[variable].mean()\n",
    "        \n",
    "        if energy == \"mid\" or energy == \"high\":\n",
    "            if task == 3 or task == 14:\n",
    "                mean *= 3 # 3 iterations of taupede\n",
    "\n",
    "        result[i] = mean; \n",
    "        result[\"total\"] += mean\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obtain_file_size( dataset_id ):\n",
    "\n",
    "    file_paths = [f\"/data/sim/IceCube/2023/generated/neutrino-generator/{dataset_id}/0000000-0000999/\",\n",
    "                  f\"/data/sim/IceCube/2023/filtered/level2/neutrino-generator/{dataset_id}/0000000-0000999/\",\n",
    "                  f\"/data/sim/IceCube/2023/filtered/level3/cascade/neutrino-generator/{dataset_id}/0000000-0000999/\",\n",
    "                  f\"/data/sim/IceCube/2023/filtered/level4/cascade/neutrino-generator/{dataset_id}/0000000-0000999/\",\n",
    "\n",
    "                  f\"/data/sim/IceCube/2023/filtered/level5/cascade/neutrino-generator/cascade/{dataset_id}/0000000-0000999/\",\n",
    "                  f\"/data/sim/IceCube/2023/filtered/level5/cascade/neutrino-generator/hybrid/{dataset_id}/0000000-0000999/\",\n",
    "                  f\"/data/sim/IceCube/2023/filtered/level5/cascade/neutrino-generator/muon/{dataset_id}/0000000-0000999/\",\n",
    "\n",
    "                  f\"/data/sim/IceCube/2023/filtered/level6/cascade/neutrino-generator/cascade/{dataset_id}/0000000-0000999/\",\n",
    "                  f\"/data/sim/IceCube/2023/filtered/level6/cascade/neutrino-generator/hybrid/{dataset_id}/0000000-0000999/\",\n",
    "                  f\"/data/sim/IceCube/2023/filtered/level6/cascade/neutrino-generator/muon/{dataset_id}/0000000-0000999/\",\n",
    "\n",
    "                  f\"/data/sim/IceCube/2023/filtered/level8/cascade/neutrino-generator/cascade/{dataset_id}/0000000-0000999/\"]\n",
    "\n",
    "    if \"low\" not in datasets_snowstorm[dataset_id][\"energy\"]:\n",
    "        file_paths += [f\"/data/sim/IceCube/2023/filtered/HESE/neutrino-generator/evtgen/{dataset_id}/0000000-0000999/\" ]\n",
    "\n",
    "    result = {\"total\" : 0}\n",
    "\n",
    "    for i,file_path in enumerate(file_paths):\n",
    "\n",
    "        sizes = []\n",
    "        for fname in os.listdir(file_path):\n",
    "            sizes.append(os.path.getsize(os.path.join(file_path, fname)))\n",
    "\n",
    "        avg_size = np.mean(sizes) / 1e9  # in GB\n",
    "        result[i] = avg_size\n",
    "\n",
    "        result[\"total\"] +=avg_size\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   dataset         type  nfiles  cpu_hours  Space (GB)\n",
      "0    23460   NuTau_high   22000      98488        2830\n",
      "1    23459    NuTau_mid    4000      41395         979\n",
      "2    23458    NuTau_low    1100      22359         616\n",
      "3    23457     NuE_high   24000     106311        3263\n",
      "4    23456      NuE_mid    4200      73399        1514\n",
      "5    23455      NuE_low     900      28297         690\n",
      "6    23454    NuMu_high   18000      91281        2824\n",
      "7    23453     NuMu_mid    5400      40019        1286\n",
      "8    23452     NuMu_low    8100      44984        3579\n",
      "9    23451  NuMu_lowlow    4000       5648         657\n",
      "10   Total                91700     552181       18238\n"
     ]
    }
   ],
   "source": [
    "# Collect rows in a list\n",
    "rows = []\n",
    "for dataset_id, info in datasets_snowstorm.items():\n",
    "\n",
    "    df = datasets_snowstorm[dataset_id][\"df\"]\n",
    "\n",
    "    energy = datasets_snowstorm[dataset_id][\"energy\"]\n",
    "\n",
    "    mean_cpu_hours = extract_mean_usage( \"time_used\",dataset_id, df, energy )\n",
    "    average_total_file_size = obtain_file_size( dataset_id )[\"total\"]\n",
    "\n",
    "    row = {\n",
    "        \"dataset\": dataset_id,\n",
    "        \"type\": f'{info[\"flavor\"]}_{info[\"energy\"]}',\n",
    "        \"nfiles\": info[\"nfiles\"],\n",
    "        \"cpu_hours\": int(info[\"nfiles\"]*mean_cpu_hours[\"total\"]), # for bright + deepcore/bright\n",
    "        \"Space (GB)\": int(info[\"nfiles\"]*average_total_file_size), # I will only save evtgen output\n",
    "    }\n",
    "    rows.append(row)\n",
    "\n",
    "# Convert list of dicts â†’ DataFrame\n",
    "df = pd.DataFrame(rows)\n",
    "\n",
    "# Add a final row with sums\n",
    "sum_row = {\n",
    "    \"dataset\": \"Total\",\n",
    "    \"type\": \"\",\n",
    "    \"nfiles\": df[\"nfiles\"].sum(),\n",
    "    \"cpu_hours\": df[\"cpu_hours\"].sum(),\n",
    "    \"Space (GB)\": df[\"Space (GB)\"].sum()\n",
    "}\n",
    "\n",
    "df = pd.concat([df, pd.DataFrame([sum_row])], ignore_index=True)\n",
    "\n",
    "print(df)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3-v4.4.1_reco-v1.1.0",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
