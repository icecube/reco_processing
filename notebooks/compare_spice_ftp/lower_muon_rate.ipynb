{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import simweights\n",
    "import pickle\n",
    "import os, sys\n",
    "import re\n",
    "import numpy as np\n",
    "import matplotlib as mat\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import matplotlib.colors as colors\n",
    "import matplotlib.gridspec as gridspec\n",
    "import pandas as pd\n",
    "import tables\n",
    "import h5py\n",
    "import math\n",
    "from scipy.stats import mstats\n",
    "import matplotlib as mpl\n",
    "import matplotlib.font_manager as font_manager\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(\"/data/user/tvaneede/GlobalFit/reco_processing/notebooks/weighting\")\n",
    "from weights import *\n",
    "from utils import *\n",
    "from selections import selection_mask\n",
    "from fonts import *\n",
    "from plot_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Append the custom module path\n",
    "sys.path.append(\"/data/user/tvaneede/GlobalFit/reco_processing\")\n",
    "\n",
    "# Import the datasets module\n",
    "from datasets import datasets\n",
    "\n",
    "# set the inputs\n",
    "reco_versions = [\"evtgen_v1_rec_v2\", \"spice_tau_reco\"]\n",
    "\n",
    "# Dynamically select the desired dataset\n",
    "simulation_datasets = {}\n",
    "for reco_version in reco_versions: simulation_datasets[reco_version] = getattr(datasets, reco_version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "livetime_yr = 11.687\n",
    "livetime_s  = livetime_yr * 365.25 * 24 * 3600 # 11.687 year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main_plotting_path = f\"/data/user/tvaneede/GlobalFit/reco_processing/notebooks/compare_spice_ftp/output\"\n",
    "os.system(f\"mkdir -p {main_plotting_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weight functions\n",
    "spline_file = '/data/ana/Diffuse/NNMFit/MCEq_splines/v1.2.1/MCEq_splines_PRI-Gaisser-H4a_INT-SIBYLL23c_allfluxes.pickle'\n",
    "\n",
    "# conventional            \n",
    "flux_keys_conv =  ['conv_antinumu','conv_numu','conv_antinue','conv_nue','conv_antinutau','conv_nutau']\n",
    "spline_object_conv = SplineHandler(spline_file, flux_keys_conv)\n",
    "conv_flux = spline_object_conv.return_weight\n",
    "generator_conv = lambda pdgid, energy, cos_zen: conv_flux(pdgid, energy, cos_zen)\n",
    "\n",
    "# prompt\n",
    "flux_keys_pr =  ['pr_antinumu','pr_numu','pr_antinue','pr_nue','pr_antinutau','pr_nutau']\n",
    "spline_object_pr = SplineHandler(spline_file, flux_keys_pr)\n",
    "pr_flux = spline_object_pr.return_weight\n",
    "generator_pr = lambda pdgid, energy, cos_zen: pr_flux(pdgid, energy, cos_zen)\n",
    "\n",
    "# astro\n",
    "gamma_astro = 2.87\n",
    "per_flavor_norm = 2.12\n",
    "def AstroFluxModel(pdgid, energy, cos_zen):\n",
    "    flux = 0.5*(per_flavor_norm*1e-18)*(energy/1e5)**-gamma_astro\n",
    "    return flux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_datasets( simulation_dataset, keys_to_merge ):\n",
    "\n",
    "    # open the files\n",
    "    for key in simulation_dataset:\n",
    "        print(f\"----- Extracting files for {key}\")\n",
    "        simulation_dataset[key]['hdf_file'] = pd.HDFStore(simulation_dataset[key]['hdf_file_path'],'r')\n",
    "        simulation_dataset[key]['weighter'] = simweights.NuGenWeighter( simulation_dataset[key]['hdf_file'] ,nfiles=simulation_dataset[key]['nfiles'])\n",
    "\n",
    "    # merging files\n",
    "    for new_key in keys_to_merge:\n",
    "        print(f\"----- Creating new key {new_key}\")\n",
    "        simulation_dataset[new_key] = {}\n",
    "        simulation_dataset[new_key]['variables'] = {}\n",
    "        simulation_dataset[new_key]['weighter'] = None\n",
    "\n",
    "        for key in keys_to_merge[new_key]:\n",
    "            \n",
    "            print(f\"Using {key}\")\n",
    "            # merge the weighters\n",
    "            if simulation_dataset[new_key]['weighter'] == None:\n",
    "                simulation_dataset[new_key]['weighter'] = simulation_dataset[key]['weighter']\n",
    "            else: simulation_dataset[new_key]['weighter'] += simulation_dataset[key]['weighter']\n",
    "\n",
    "    # calculate weights\n",
    "    for key in simulation_dataset:\n",
    "        simulation_dataset[key]['weights_astro'] = simulation_dataset[key][\"weighter\"].get_weights(AstroFluxModel) * livetime_s\n",
    "        simulation_dataset[key]['weights_conv'] = simulation_dataset[key][\"weighter\"].get_weights(generator_conv) * livetime_s\n",
    "        simulation_dataset[key]['weights_pr'] = simulation_dataset[key][\"weighter\"].get_weights(generator_pr) * livetime_s\n",
    "\n",
    "\n",
    "    return simulation_dataset\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys_to_merge = {}\n",
    "\n",
    "keys_to_merge[\"evtgen_v1_rec_v2\"] = {\n",
    "    \"NuE\" : [\"NuE_midE\", \"NuE_highE\"],\n",
    "    \"NuMu\" : [\"NuMu_midE\", \"NuMu_highE\"],\n",
    "    \"NuTau\" : [\"NuTau_midE\", \"NuTau_highE\"],\n",
    "    \"NuAll\" : ['NuE', \"NuMu\", \"NuTau\"],\n",
    "}\n",
    "\n",
    "keys_to_merge[\"v2\"] = {\n",
    "    \"NuE\" : [\"NuE_midE\", \"NuE_highE\"],\n",
    "    \"NuMu\" : [\"NuMu_midE\", \"NuMu_highE\"],\n",
    "    \"NuTau\" : [\"NuTau_midE\", \"NuTau_highE\"],\n",
    "    \"NuAll\" : ['NuE', \"NuMu\", \"NuTau\"],\n",
    "}\n",
    "\n",
    "keys_to_merge[\"spice_tau_reco\"] = {\n",
    "    \"NuE\" : [\"NuE_midE1\", \"NuE_highE1\", \"NuE_midE2\", \"NuE_highE2\"],\n",
    "    \"NuMu\" : [\"NuMu_midE1\", \"NuMu_highE1\",\"NuMu_midE2\", \"NuMu_highE2\"],\n",
    "    \"NuTau\" : [\"NuTau_midE1\", \"NuTau_highE1\",\"NuTau_midE2\", \"NuTau_highE2\"],\n",
    "    \"NuAll\" : ['NuE', \"NuMu\", \"NuTau\"],\n",
    "\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- Extracting files for NuTau_midE\n",
      "----- Extracting files for NuTau_highE\n",
      "----- Extracting files for NuE_midE\n",
      "----- Extracting files for NuE_highE\n",
      "----- Extracting files for NuMu_midE\n",
      "----- Extracting files for NuMu_highE\n",
      "----- Creating new key NuE\n",
      "Using NuE_midE\n",
      "Using NuE_highE\n",
      "----- Creating new key NuMu\n",
      "Using NuMu_midE\n",
      "Using NuMu_highE\n",
      "----- Creating new key NuTau\n",
      "Using NuTau_midE\n",
      "Using NuTau_highE\n",
      "----- Creating new key NuAll\n",
      "Using NuE\n",
      "Using NuMu\n",
      "Using NuTau\n",
      "----- Extracting files for NuTau_midE1\n",
      "----- Extracting files for NuTau_highE1\n",
      "----- Extracting files for NuTau_midE2\n",
      "----- Extracting files for NuTau_highE2\n",
      "----- Extracting files for NuE_midE1\n",
      "----- Extracting files for NuE_highE1\n",
      "----- Extracting files for NuE_midE2\n",
      "----- Extracting files for NuE_highE2\n",
      "----- Extracting files for NuMu_midE1\n",
      "----- Extracting files for NuMu_highE1\n",
      "----- Extracting files for NuMu_midE2\n",
      "----- Extracting files for NuMu_highE2\n",
      "----- Creating new key NuE\n",
      "Using NuE_midE1\n",
      "Using NuE_highE1\n",
      "Using NuE_midE2\n",
      "Using NuE_highE2\n",
      "----- Creating new key NuMu\n",
      "Using NuMu_midE1\n",
      "Using NuMu_highE1\n",
      "Using NuMu_midE2\n",
      "Using NuMu_highE2\n",
      "----- Creating new key NuTau\n",
      "Using NuTau_midE1\n",
      "Using NuTau_highE1\n",
      "Using NuTau_midE2\n",
      "Using NuTau_highE2\n",
      "----- Creating new key NuAll\n",
      "Using NuE\n",
      "Using NuMu\n",
      "Using NuTau\n"
     ]
    }
   ],
   "source": [
    "for key in simulation_datasets: simulation_datasets[key] = open_datasets( simulation_datasets[key], keys_to_merge[key] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     astro_NuE    astro_NuMu   astro_NuTau          conv        prompt\n",
      "evtgen_v1_rec_v2  56.20 ± 0.54  14.74 ± 0.22  33.91 ± 0.39  32.36 ± 0.97  12.47 ± 0.10\n",
      "spice_tau_reco    56.77 ± 0.56  20.42 ± 0.22  34.89 ± 0.43  38.77 ± 0.95  13.41 ± 0.11\n"
     ]
    }
   ],
   "source": [
    "data = {}\n",
    "\n",
    "for key in simulation_datasets:\n",
    "\n",
    "    simulation_dataset = simulation_datasets[key]\n",
    "\n",
    "    channel_data = {}\n",
    "\n",
    "    for flavor in ['NuE', \"NuMu\", \"NuTau\"]:\n",
    "        weights = simulation_dataset[flavor][\"weighter\"].get_weights(AstroFluxModel) * livetime_s\n",
    "        rate = np.sum(weights)\n",
    "        error = np.sqrt(np.sum(weights**2))\n",
    "        channel_data[f\"astro_{flavor}\"] = f\"{rate:.2f} ± {error:.2f}\"\n",
    "\n",
    "    # Conventional\n",
    "    flavor = \"NuAll\"\n",
    "    weights_conv = simulation_dataset[flavor][\"weighter\"].get_weights(generator_conv) * livetime_s\n",
    "    rate_conv = np.sum(weights_conv)\n",
    "    err_conv = np.sqrt(np.sum(weights_conv**2))\n",
    "    channel_data[\"conv\"] = f\"{rate_conv:.2f} ± {err_conv:.2f}\"\n",
    "\n",
    "    # Prompt\n",
    "    weights_prompt = simulation_dataset[flavor][\"weighter\"].get_weights(generator_pr) * livetime_s\n",
    "    rate_prompt = np.sum(weights_prompt)\n",
    "    err_prompt = np.sqrt(np.sum(weights_prompt**2))\n",
    "    channel_data[\"prompt\"] = f\"{rate_prompt:.2f} ± {err_prompt:.2f}\"\n",
    "\n",
    "    data[key] = channel_data\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame.from_dict(data, orient='index')\n",
    "\n",
    "# Optional: specify column order\n",
    "columns_order = [f\"astro_{flavor}\" for flavor in ['NuE', 'NuMu', 'NuTau']] + [\"conv\", \"prompt\"]\n",
    "df = df[columns_order]\n",
    "\n",
    "# Display as string table\n",
    "print(df.to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 astro_NuMu_midE astro_NuMu_highE\n",
      "evtgen_v1_rec_v2    13.92 ± 0.21      0.82 ± 0.04\n",
      "               astro_NuMu_midE1 astro_NuMu_highE1 astro_NuMu_midE2 astro_NuMu_highE2\n",
      "spice_tau_reco     19.28 ± 0.41       1.22 ± 0.04     19.18 ± 0.27       1.21 ± 0.03\n"
     ]
    }
   ],
   "source": [
    "for key in simulation_datasets:\n",
    "\n",
    "    data = {}\n",
    "    simulation_dataset = simulation_datasets[key]\n",
    "\n",
    "    channel_data = {}\n",
    "\n",
    "    for flavor in keys_to_merge[key][\"NuMu\"]:\n",
    "        weights = simulation_dataset[flavor][\"weighter\"].get_weights(AstroFluxModel) * livetime_s\n",
    "        rate = np.sum(weights)\n",
    "        error = np.sqrt(np.sum(weights**2))\n",
    "        channel_data[f\"astro_{flavor}\"] = f\"{rate:.2f} ± {error:.2f}\"\n",
    "\n",
    "    data[key] = channel_data\n",
    "\n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame.from_dict(data, orient='index')\n",
    "\n",
    "    # Display as string table\n",
    "    print(df.to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I seem to have only 72-67%. Let's see if v2 of the reco is fine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import datasets\n",
    "import importlib\n",
    "\n",
    "importlib.reload(datasets)\n",
    "\n",
    "# set the inputs\n",
    "reco_versions = [\"v2\", \"spice_tau_reco\"]\n",
    "\n",
    "# Dynamically select the desired dataset\n",
    "simulation_datasets = {}\n",
    "for reco_version in reco_versions: simulation_datasets[reco_version] = getattr(datasets, reco_version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- Extracting files for NuTau_midE\n",
      "----- Extracting files for NuTau_highE\n",
      "----- Extracting files for NuE_midE\n",
      "----- Extracting files for NuE_highE\n",
      "----- Extracting files for NuMu_midE\n",
      "----- Extracting files for NuMu_highE\n",
      "----- Creating new key NuE\n",
      "Using NuE_midE\n",
      "Using NuE_highE\n",
      "----- Creating new key NuMu\n",
      "Using NuMu_midE\n",
      "Using NuMu_highE\n",
      "----- Creating new key NuTau\n",
      "Using NuTau_midE\n",
      "Using NuTau_highE\n",
      "----- Creating new key NuAll\n",
      "Using NuE\n",
      "Using NuMu\n",
      "Using NuTau\n",
      "----- Extracting files for NuTau_midE1\n",
      "----- Extracting files for NuTau_highE1\n",
      "----- Extracting files for NuTau_midE2\n",
      "----- Extracting files for NuTau_highE2\n",
      "----- Extracting files for NuE_midE1\n",
      "----- Extracting files for NuE_highE1\n",
      "----- Extracting files for NuE_midE2\n",
      "----- Extracting files for NuE_highE2\n",
      "----- Extracting files for NuMu_midE1\n",
      "----- Extracting files for NuMu_highE1\n",
      "----- Extracting files for NuMu_midE2\n",
      "----- Extracting files for NuMu_highE2\n",
      "----- Creating new key NuE\n",
      "Using NuE_midE1\n",
      "Using NuE_highE1\n",
      "Using NuE_midE2\n",
      "Using NuE_highE2\n",
      "----- Creating new key NuMu\n",
      "Using NuMu_midE1\n",
      "Using NuMu_highE1\n",
      "Using NuMu_midE2\n",
      "Using NuMu_highE2\n",
      "----- Creating new key NuTau\n",
      "Using NuTau_midE1\n",
      "Using NuTau_highE1\n",
      "Using NuTau_midE2\n",
      "Using NuTau_highE2\n",
      "----- Creating new key NuAll\n",
      "Using NuE\n",
      "Using NuMu\n",
      "Using NuTau\n"
     ]
    }
   ],
   "source": [
    "for key in simulation_datasets: simulation_datasets[key] = open_datasets( simulation_datasets[key], keys_to_merge[key] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   astro_NuMu_midE astro_NuMu_highE\n",
      "v2    13.92 ± 0.21      0.82 ± 0.04\n",
      "               astro_NuMu_midE1 astro_NuMu_highE1 astro_NuMu_midE2 astro_NuMu_highE2\n",
      "spice_tau_reco     19.28 ± 0.41       1.22 ± 0.04     19.18 ± 0.27       1.21 ± 0.03\n"
     ]
    }
   ],
   "source": [
    "for key in simulation_datasets:\n",
    "\n",
    "    data = {}\n",
    "    simulation_dataset = simulation_datasets[key]\n",
    "\n",
    "    channel_data = {}\n",
    "\n",
    "    for flavor in keys_to_merge[key][\"NuMu\"]:\n",
    "        weights = simulation_dataset[flavor][\"weighter\"].get_weights(AstroFluxModel) * livetime_s\n",
    "        rate = np.sum(weights)\n",
    "        error = np.sqrt(np.sum(weights**2))\n",
    "        channel_data[f\"astro_{flavor}\"] = f\"{rate:.2f} ± {error:.2f}\"\n",
    "\n",
    "    data[key] = channel_data\n",
    "\n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame.from_dict(data, orient='index')\n",
    "\n",
    "    # Display as string table\n",
    "    print(df.to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also missing! Let's take a look at ftp_l3casc and do a cut myself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import datasets\n",
    "import importlib\n",
    "\n",
    "importlib.reload(datasets)\n",
    "\n",
    "# set the inputs\n",
    "reco_versions = [\"ftp_l3casc\", \"spice_l3casc\"]\n",
    "\n",
    "# Dynamically select the desired dataset\n",
    "simulation_datasets = {}\n",
    "for reco_version in reco_versions: simulation_datasets[reco_version] = getattr(datasets, reco_version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- Extracting files for NuTau_midE1\n",
      "----- Extracting files for NuTau_midE2\n",
      "----- Extracting files for NuTau_highE1\n",
      "----- Extracting files for NuTau_highE2\n",
      "----- Extracting files for NuE_midE1\n",
      "----- Extracting files for NuE_midE2\n",
      "----- Extracting files for NuE_highE1\n",
      "----- Extracting files for NuE_highE2\n",
      "----- Extracting files for NuMu_midE1\n",
      "----- Extracting files for NuMu_midE2\n",
      "----- Extracting files for NuMu_highE1\n",
      "----- Extracting files for NuMu_highE2\n",
      "----- Creating new key NuE\n",
      "Using NuE_midE1\n",
      "Using NuE_highE1\n",
      "Using NuE_midE2\n",
      "Using NuE_highE2\n",
      "----- Creating new key NuMu\n",
      "Using NuMu_midE1\n",
      "Using NuMu_highE1\n",
      "Using NuMu_midE2\n",
      "Using NuMu_highE2\n",
      "----- Creating new key NuTau\n",
      "Using NuTau_midE1\n",
      "Using NuTau_highE1\n",
      "Using NuTau_midE2\n",
      "Using NuTau_highE2\n",
      "----- Creating new key NuAll\n",
      "Using NuE\n",
      "Using NuMu\n",
      "Using NuTau\n",
      "----- Extracting files for NuTau_midE1\n",
      "----- Extracting files for NuTau_highE1\n",
      "----- Extracting files for NuTau_midE2\n",
      "----- Extracting files for NuTau_highE2\n",
      "----- Extracting files for NuE_midE1\n",
      "----- Extracting files for NuE_highE1\n",
      "----- Extracting files for NuE_midE2\n",
      "----- Extracting files for NuE_highE2\n",
      "----- Extracting files for NuMu_midE1\n",
      "----- Extracting files for NuMu_highE1\n",
      "----- Extracting files for NuMu_midE2\n",
      "----- Extracting files for NuMu_highE2\n",
      "----- Creating new key NuE\n",
      "Using NuE_midE1\n",
      "Using NuE_highE1\n",
      "Using NuE_midE2\n",
      "Using NuE_highE2\n",
      "----- Creating new key NuMu\n",
      "Using NuMu_midE1\n",
      "Using NuMu_highE1\n",
      "Using NuMu_midE2\n",
      "Using NuMu_highE2\n",
      "----- Creating new key NuTau\n",
      "Using NuTau_midE1\n",
      "Using NuTau_highE1\n",
      "Using NuTau_midE2\n",
      "Using NuTau_highE2\n",
      "----- Creating new key NuAll\n",
      "Using NuE\n",
      "Using NuMu\n",
      "Using NuTau\n"
     ]
    }
   ],
   "source": [
    "keys_to_merge[\"ftp_l3casc\"] = {\n",
    "    \"NuE\" : [\"NuE_midE1\", \"NuE_highE1\", \"NuE_midE2\", \"NuE_highE2\"],\n",
    "    \"NuMu\" : [\"NuMu_midE1\", \"NuMu_highE1\",\"NuMu_midE2\", \"NuMu_highE2\"],\n",
    "    \"NuTau\" : [\"NuTau_midE1\", \"NuTau_highE1\",\"NuTau_midE2\", \"NuTau_highE2\"],\n",
    "    \"NuAll\" : ['NuE', \"NuMu\", \"NuTau\"],   \n",
    "}\n",
    "\n",
    "keys_to_merge[\"spice_l3casc\"] = {\n",
    "    \"NuE\" : [\"NuE_midE1\", \"NuE_highE1\", \"NuE_midE2\", \"NuE_highE2\"],\n",
    "    \"NuMu\" : [\"NuMu_midE1\", \"NuMu_highE1\",\"NuMu_midE2\", \"NuMu_highE2\"],\n",
    "    \"NuTau\" : [\"NuTau_midE1\", \"NuTau_highE1\",\"NuTau_midE2\", \"NuTau_highE2\"],\n",
    "    \"NuAll\" : ['NuE', \"NuMu\", \"NuTau\"],\n",
    "}\n",
    "\n",
    "for key in simulation_datasets: simulation_datasets[key] = open_datasets( simulation_datasets[key], keys_to_merge[key] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  astro_NuMu_midE1 astro_NuMu_highE1 astro_NuMu_midE2 astro_NuMu_highE2\n",
      "ftp_l3casc           890.37 ± 1.65       8.84 ± 0.05    894.52 ± 1.70       8.86 ± 0.06\n",
      "ftp_l3casc_masked     13.94 ± 0.13       0.83 ± 0.01     14.18 ± 0.14       0.84 ± 0.02\n"
     ]
    }
   ],
   "source": [
    "data = {}\n",
    "\n",
    "key = \"ftp_l3casc\"\n",
    "\n",
    "simulation_dataset = simulation_datasets[key]\n",
    "\n",
    "channel_data = {}\n",
    "channel_data_masked = {}\n",
    "\n",
    "for flavor in keys_to_merge[key][\"NuMu\"]:\n",
    "    weights = simulation_dataset[flavor][\"weighter\"].get_weights(AstroFluxModel) * livetime_s\n",
    "    rate = np.sum(weights)\n",
    "    error = np.sqrt(np.sum(weights**2))\n",
    "    channel_data[f\"astro_{flavor}\"] = f\"{rate:.2f} ± {error:.2f}\"\n",
    "\n",
    "    HESE_CausalQTot = simulation_dataset[flavor][\"hdf_file\"][\"HESE_CausalQTot\"].value\n",
    "    mask = HESE_CausalQTot > 6000\n",
    "    rate_masked = np.sum(weights[mask])\n",
    "    error_masked = np.sqrt(np.sum(weights[mask]**2))\n",
    "    channel_data_masked[f\"astro_{flavor}\"] = f\"{rate_masked:.2f} ± {error_masked:.2f}\"\n",
    "\n",
    "data[key] = channel_data\n",
    "data[f\"{key}_masked\"] = channel_data_masked\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame.from_dict(data, orient='index')\n",
    "\n",
    "# Display as string table\n",
    "print(df.to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seems to be the same amount of events missing. Lets check one dataset of 0000000-0000999"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rate_NuMu_midE 888.3088102238869 14.876935899724039\n",
      "rate_NuMu_highE 8.933702850310775 0.9000591852292844\n"
     ]
    }
   ],
   "source": [
    "file_path_NuMu_midE = \"/data/user/tvaneede/GlobalFit/reco_processing/hdf/output/ftp_l3casc/NuMu_22645_0000000-0000999.h5\"\n",
    "hdf_NuMu_midE = pd.HDFStore(file_path_NuMu_midE,'r')\n",
    "nfiles_NuMu_midE = 1000\n",
    "weighter_NuMu_midE = simweights.NuGenWeighter( hdf_NuMu_midE, nfiles=nfiles_NuMu_midE)\n",
    "weights_NuMu_midE = weighter_NuMu_midE.get_weights(AstroFluxModel) * livetime_s\n",
    "rate_NuMu_midE = np.sum(weights_NuMu_midE)\n",
    "mask_NuMu_midE = hdf_NuMu_midE[\"HESE_CausalQTot\"].value > 6000\n",
    "rate_masked_NuMu_midE = np.sum(weights_NuMu_midE[mask_NuMu_midE])\n",
    "\n",
    "print(\"rate_NuMu_midE\", rate_NuMu_midE, rate_masked_NuMu_midE)\n",
    "\n",
    "file_path_NuMu_highE = \"/data/user/tvaneede/GlobalFit/reco_processing/hdf/output/ftp_l3casc/NuMu_22644_0000000-0000999.h5\"\n",
    "hdf_NuMu_highE = pd.HDFStore(file_path_NuMu_highE,'r')\n",
    "nfiles_NuMu_highE = 1000\n",
    "weighter_NuMu_highE = simweights.NuGenWeighter( hdf_NuMu_highE, nfiles=nfiles_NuMu_highE)\n",
    "weights_NuMu_highE = weighter_NuMu_highE.get_weights(AstroFluxModel) * livetime_s\n",
    "rate_NuMu_highE = np.sum(weights_NuMu_highE)\n",
    "mask_NuMu_highE = hdf_NuMu_highE[\"HESE_CausalQTot\"].value > 6000\n",
    "rate_masked_NuMu_highE = np.sum(weights_NuMu_highE[mask_NuMu_highE])\n",
    "\n",
    "print(\"rate_NuMu_highE\", rate_NuMu_highE, rate_masked_NuMu_highE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am really starting to believe that we actually have fewer muon neutrinos at hese level for the ftp-v3 simulations. Lets make a hdf of the spice files at cascade level to see if there is a difference there as well?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Turns out one reco file was corrupted, so the hdf of that group of files was broken, see tools/find_error_in_log.py\n",
    "\n",
    "Missing jobs: 0\n",
    "\n",
    "Error jobs: 1\n",
    "{'NuMu_22043_0000000-0000999': {'LOGDIR': '/scratch/tvaneede/reco/hdf_taupede_tianlu/spice_l3casc/hdf_dag_spice_l3casc/logs', 'JOBID': 'NuMu_22043_0000000-0000999', 'INPATH': '/data/sim/IceCube/2020/filtered/level3/cascade/neutrino-generator/22043/0000000-0000999', 'OUTFILE': '/data/user/tvaneede/GlobalFit/reco_processing/hdf/output/spice_l3casc/NuMu_22043_0000000-0000999.h5'}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    astro_NuMu_midE1 astro_NuMu_highE1 astro_NuMu_midE2 astro_NuMu_highE2\n",
      "spice_l3casc           917.54 ± 4.53       9.31 ± 0.13    919.78 ± 2.97       9.19 ± 0.08\n",
      "spice_l3casc_masked     14.01 ± 0.36       0.81 ± 0.04     13.77 ± 0.23       0.82 ± 0.02\n"
     ]
    }
   ],
   "source": [
    "data = {}\n",
    "\n",
    "key = \"spice_l3casc\"\n",
    "\n",
    "simulation_dataset = simulation_datasets[key]\n",
    "\n",
    "channel_data = {}\n",
    "channel_data_masked = {}\n",
    "\n",
    "for flavor in [\"NuMu_midE1\",\"NuMu_highE1\",\"NuMu_midE2\", \"NuMu_highE2\"]: # \"NuMu_midE1\" was/is corrupt\n",
    "    weights = simulation_dataset[flavor][\"weighter\"].get_weights(AstroFluxModel) * livetime_s\n",
    "    rate = np.sum(weights)\n",
    "    error = np.sqrt(np.sum(weights**2))\n",
    "    channel_data[f\"astro_{flavor}\"] = f\"{rate:.2f} ± {error:.2f}\"\n",
    "\n",
    "    HESE_CausalQTot = simulation_dataset[flavor][\"hdf_file\"][\"HESE_CausalQTot\"].value\n",
    "    mask = HESE_CausalQTot > 6000\n",
    "    rate_masked = np.sum(weights[mask])\n",
    "    error_masked = np.sqrt(np.sum(weights[mask]**2))\n",
    "    channel_data_masked[f\"astro_{flavor}\"] = f\"{rate_masked:.2f} ± {error_masked:.2f}\"\n",
    "\n",
    "data[key] = channel_data\n",
    "data[f\"{key}_masked\"] = channel_data_masked\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame.from_dict(data, orient='index')\n",
    "\n",
    "# Display as string table\n",
    "print(df.to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow!! It turns out, if I do the cut myself on spice, I get the same number. How did Neha get higher values? Probably due to her definitions in \n",
    "https://github.com/icecube/wg-diffuse/blob/2023_GlobalFit_Flavor/Ternary_Classifier/segments/VHESelfVeto.py\n",
    "\n",
    "I made new datasets in spice_l3casc_qtot that contains both CausalQTot calculations. Let's see what happens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import datasets\n",
    "import importlib\n",
    "\n",
    "importlib.reload(datasets)\n",
    "\n",
    "# set the inputs\n",
    "reco_versions = [\"ftp_l3casc\", \"spice_l3casc_qtot\"]\n",
    "\n",
    "# Dynamically select the desired dataset\n",
    "simulation_datasets = {}\n",
    "for reco_version in reco_versions: simulation_datasets[reco_version] = getattr(datasets, reco_version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- Extracting files for NuTau_midE1\n",
      "----- Extracting files for NuTau_midE2\n",
      "----- Extracting files for NuTau_highE1\n",
      "----- Extracting files for NuTau_highE2\n",
      "----- Extracting files for NuE_midE1\n",
      "----- Extracting files for NuE_midE2\n",
      "----- Extracting files for NuE_highE1\n",
      "----- Extracting files for NuE_highE2\n",
      "----- Extracting files for NuMu_midE1\n",
      "----- Extracting files for NuMu_midE2\n",
      "----- Extracting files for NuMu_highE1\n",
      "----- Extracting files for NuMu_highE2\n",
      "----- Creating new key NuE\n",
      "Using NuE_midE1\n",
      "Using NuE_highE1\n",
      "Using NuE_midE2\n",
      "Using NuE_highE2\n",
      "----- Creating new key NuMu\n",
      "Using NuMu_midE1\n",
      "Using NuMu_highE1\n",
      "Using NuMu_midE2\n",
      "Using NuMu_highE2\n",
      "----- Creating new key NuTau\n",
      "Using NuTau_midE1\n",
      "Using NuTau_highE1\n",
      "Using NuTau_midE2\n",
      "Using NuTau_highE2\n",
      "----- Creating new key NuAll\n",
      "Using NuE\n",
      "Using NuMu\n",
      "Using NuTau\n",
      "----- Extracting files for NuTau_midE1\n",
      "----- Extracting files for NuTau_highE1\n",
      "----- Extracting files for NuTau_midE2\n",
      "----- Extracting files for NuTau_highE2\n",
      "----- Extracting files for NuE_midE1\n",
      "----- Extracting files for NuE_highE1\n",
      "----- Extracting files for NuE_midE2\n",
      "----- Extracting files for NuE_highE2\n",
      "----- Extracting files for NuMu_midE1\n",
      "----- Extracting files for NuMu_highE1\n",
      "----- Extracting files for NuMu_midE2\n",
      "----- Extracting files for NuMu_highE2\n",
      "----- Creating new key NuE\n",
      "Using NuE_midE1\n",
      "Using NuE_highE1\n",
      "Using NuE_midE2\n",
      "Using NuE_highE2\n",
      "----- Creating new key NuMu\n",
      "Using NuMu_midE1\n",
      "Using NuMu_highE1\n",
      "Using NuMu_midE2\n",
      "Using NuMu_highE2\n",
      "----- Creating new key NuTau\n",
      "Using NuTau_midE1\n",
      "Using NuTau_highE1\n",
      "Using NuTau_midE2\n",
      "Using NuTau_highE2\n",
      "----- Creating new key NuAll\n",
      "Using NuE\n",
      "Using NuMu\n",
      "Using NuTau\n"
     ]
    }
   ],
   "source": [
    "keys_to_merge[\"ftp_l3casc\"] = {\n",
    "    \"NuE\" : [\"NuE_midE1\", \"NuE_highE1\", \"NuE_midE2\", \"NuE_highE2\"],\n",
    "    \"NuMu\" : [\"NuMu_midE1\", \"NuMu_highE1\",\"NuMu_midE2\", \"NuMu_highE2\"],\n",
    "    \"NuTau\" : [\"NuTau_midE1\", \"NuTau_highE1\",\"NuTau_midE2\", \"NuTau_highE2\"],\n",
    "    \"NuAll\" : ['NuE', \"NuMu\", \"NuTau\"],   \n",
    "}\n",
    "\n",
    "keys_to_merge[\"spice_l3casc_qtot\"] = {\n",
    "    \"NuE\" : [\"NuE_midE1\", \"NuE_highE1\", \"NuE_midE2\", \"NuE_highE2\"],\n",
    "    \"NuMu\" : [\"NuMu_midE1\", \"NuMu_highE1\",\"NuMu_midE2\", \"NuMu_highE2\"],\n",
    "    \"NuTau\" : [\"NuTau_midE1\", \"NuTau_highE1\",\"NuTau_midE2\", \"NuTau_highE2\"],\n",
    "    \"NuAll\" : ['NuE', \"NuMu\", \"NuTau\"],\n",
    "}\n",
    "\n",
    "for key in simulation_datasets: simulation_datasets[key] = open_datasets( simulation_datasets[key], keys_to_merge[key] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                              astro_NuMu_midE1 astro_NuMu_highE1 astro_NuMu_midE2 astro_NuMu_highE2\n",
      "spice_l3casc_qtot                917.54 ± 4.53       9.31 ± 0.13    919.78 ± 2.97       9.19 ± 0.08\n",
      "spice_l3casc_qtot_masked          14.01 ± 0.36       0.81 ± 0.04     13.77 ± 0.23       0.82 ± 0.02\n",
      "spice_l3casc_qtot_masked_neha     14.01 ± 0.36       0.81 ± 0.04     13.77 ± 0.23       0.82 ± 0.02\n"
     ]
    }
   ],
   "source": [
    "data = {}\n",
    "\n",
    "key = \"spice_l3casc_qtot\"\n",
    "\n",
    "simulation_dataset = simulation_datasets[key]\n",
    "\n",
    "channel_data = {}\n",
    "channel_data_masked = {}\n",
    "channel_data_masked_neha = {}\n",
    "\n",
    "for flavor in [\"NuMu_midE1\",\"NuMu_highE1\",\"NuMu_midE2\", \"NuMu_highE2\"]: # \"NuMu_midE1\" was/is corrupt\n",
    "    weights = simulation_dataset[flavor][\"weighter\"].get_weights(AstroFluxModel) * livetime_s\n",
    "    rate = np.sum(weights)\n",
    "    error = np.sqrt(np.sum(weights**2))\n",
    "    channel_data[f\"astro_{flavor}\"] = f\"{rate:.2f} ± {error:.2f}\"\n",
    "\n",
    "    HESE_CausalQTot = simulation_dataset[flavor][\"hdf_file\"][\"HESE_CausalQTot\"].value\n",
    "    mask = HESE_CausalQTot > 6000\n",
    "    rate_masked = np.sum(weights[mask])\n",
    "    error_masked = np.sqrt(np.sum(weights[mask]**2))\n",
    "    channel_data_masked[f\"astro_{flavor}\"] = f\"{rate_masked:.2f} ± {error_masked:.2f}\"\n",
    "\n",
    "    CausalQTot = simulation_dataset[flavor][\"hdf_file\"][\"CausalQTot\"].value\n",
    "    VHESelfVeto = simulation_dataset[flavor][\"hdf_file\"][\"VHESelfVeto\"].value\n",
    "    mask = (CausalQTot > 6000) & (VHESelfVeto == False)\n",
    "    rate_masked = np.sum(weights[mask])\n",
    "    error_masked = np.sqrt(np.sum(weights[mask]**2))\n",
    "    channel_data_masked_neha[f\"astro_{flavor}\"] = f\"{rate_masked:.2f} ± {error_masked:.2f}\"\n",
    "\n",
    "data[key] = channel_data\n",
    "data[f\"{key}_masked\"] = channel_data_masked\n",
    "data[f\"{key}_masked_neha\"] = channel_data_masked_neha\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame.from_dict(data, orient='index')\n",
    "\n",
    "# Display as string table\n",
    "print(df.to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interesting. It seems that both definitions get the exact same events. Why does spice_tau_reco have more events?\n",
    "\n",
    "Let's take a look at one file and select some events."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_events 101608 masked_events 4590 4590\n",
      "total_events 7128 masked_events 7128\n"
     ]
    }
   ],
   "source": [
    "file_path_l3 = \"/data/user/tvaneede/GlobalFit/reco_processing/hdf/output/spice_l3casc_nehaqtot/NuMu_22043_0000000-0000999.h5\"\n",
    "hdf_l3 = pd.HDFStore(file_path_l3,'r')\n",
    "mask_l3 = hdf_l3[\"HESE_CausalQTot\"].value > 6000\n",
    "total_l3 = len(hdf_l3[\"I3EventHeader\"])\n",
    "masked_l3 = len(hdf_l3[\"I3EventHeader\"][mask_l3])\n",
    "mask_l3_neha = (hdf_l3[\"CausalQTot\"].value > 6000) & (hdf_l3[\"VHESelfVeto\"].value == False)\n",
    "masked_l3_neha = len(hdf_l3[\"I3EventHeader\"][mask_l3_neha])\n",
    "\n",
    "print(\"total_events\", total_l3, \"masked_events\", masked_l3, masked_l3_neha)\n",
    "\n",
    "file_path_reco = \"/data/user/tvaneede/GlobalFit/reco_processing/hdf/output/spice_tau_reco/NuMu_22043_0000000-0000999.h5\"\n",
    "hdf_reco = pd.HDFStore(file_path_reco,'r')\n",
    "total_reco = len(hdf_reco[\"I3EventHeader\"])\n",
    "mask_reco_neha = (hdf_reco[\"CausalQTot\"].value > 6000) & (hdf_reco[\"VHESelfVeto\"].value == False)\n",
    "masked_reco_neha = len(hdf_reco[\"I3EventHeader\"][mask_reco_neha])\n",
    "\n",
    "print(\"total_events\", total_reco, \"masked_events\", masked_reco_neha)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of events only in reco after mask: 2538\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Run</th>\n",
       "      <th>Event</th>\n",
       "      <th>SubEvent</th>\n",
       "      <th>SubEventStream</th>\n",
       "      <th>exists</th>\n",
       "      <th>time_start_utc_daq</th>\n",
       "      <th>time_start_mjd</th>\n",
       "      <th>time_end_utc_daq</th>\n",
       "      <th>time_end_mjd</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>2204300670</td>\n",
       "      <td>89</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>130612472805802469</td>\n",
       "      <td>59000.171844</td>\n",
       "      <td>130612472806023139</td>\n",
       "      <td>59000.171844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>226</th>\n",
       "      <td>2204300612</td>\n",
       "      <td>99</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>130612472805775229</td>\n",
       "      <td>59000.171844</td>\n",
       "      <td>130612472806005409</td>\n",
       "      <td>59000.171844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>274</th>\n",
       "      <td>2204300304</td>\n",
       "      <td>72</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>130612472805729339</td>\n",
       "      <td>59000.171844</td>\n",
       "      <td>130612472805933649</td>\n",
       "      <td>59000.171844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284</th>\n",
       "      <td>2204300574</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>130612472805725099</td>\n",
       "      <td>59000.171844</td>\n",
       "      <td>130612472805939449</td>\n",
       "      <td>59000.171844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>616</th>\n",
       "      <td>2204300282</td>\n",
       "      <td>57</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>130612472805790819</td>\n",
       "      <td>59000.171844</td>\n",
       "      <td>130612472806004239</td>\n",
       "      <td>59000.171844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>777</th>\n",
       "      <td>2204300667</td>\n",
       "      <td>83</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>130612472805704679</td>\n",
       "      <td>59000.171844</td>\n",
       "      <td>130612472805944849</td>\n",
       "      <td>59000.171844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1147</th>\n",
       "      <td>2204300424</td>\n",
       "      <td>32</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>130612472805716559</td>\n",
       "      <td>59000.171844</td>\n",
       "      <td>130612472805945729</td>\n",
       "      <td>59000.171844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1419</th>\n",
       "      <td>2204300455</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>130612472805719919</td>\n",
       "      <td>59000.171844</td>\n",
       "      <td>130612472805937009</td>\n",
       "      <td>59000.171844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1441</th>\n",
       "      <td>2204300342</td>\n",
       "      <td>81</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>130612472805710669</td>\n",
       "      <td>59000.171844</td>\n",
       "      <td>130612472805946249</td>\n",
       "      <td>59000.171844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1463</th>\n",
       "      <td>2204300535</td>\n",
       "      <td>67</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>130612472810244899</td>\n",
       "      <td>59000.171844</td>\n",
       "      <td>130612472810460059</td>\n",
       "      <td>59000.171844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1713</th>\n",
       "      <td>2204300638</td>\n",
       "      <td>67</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>130612472956430029</td>\n",
       "      <td>59000.171844</td>\n",
       "      <td>130612472956657199</td>\n",
       "      <td>59000.171844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1753</th>\n",
       "      <td>2204300078</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>130612472805797349</td>\n",
       "      <td>59000.171844</td>\n",
       "      <td>130612472806057329</td>\n",
       "      <td>59000.171844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1783</th>\n",
       "      <td>2204300497</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>130612472805752779</td>\n",
       "      <td>59000.171844</td>\n",
       "      <td>130612472805946559</td>\n",
       "      <td>59000.171844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2478</th>\n",
       "      <td>2204300415</td>\n",
       "      <td>97</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>130612472805811819</td>\n",
       "      <td>59000.171844</td>\n",
       "      <td>130612472806044339</td>\n",
       "      <td>59000.171844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2760</th>\n",
       "      <td>2204300462</td>\n",
       "      <td>68</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>130612472805721549</td>\n",
       "      <td>59000.171844</td>\n",
       "      <td>130612472805956889</td>\n",
       "      <td>59000.171844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2814</th>\n",
       "      <td>2204300646</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>130612472805721129</td>\n",
       "      <td>59000.171844</td>\n",
       "      <td>130612472805943029</td>\n",
       "      <td>59000.171844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2825</th>\n",
       "      <td>2204300295</td>\n",
       "      <td>50</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>130612472805751749</td>\n",
       "      <td>59000.171844</td>\n",
       "      <td>130612472805965189</td>\n",
       "      <td>59000.171844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3160</th>\n",
       "      <td>2204300653</td>\n",
       "      <td>66</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>130612472805733669</td>\n",
       "      <td>59000.171844</td>\n",
       "      <td>130612472805958429</td>\n",
       "      <td>59000.171844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3319</th>\n",
       "      <td>2204300356</td>\n",
       "      <td>88</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>130612472805717329</td>\n",
       "      <td>59000.171844</td>\n",
       "      <td>130612472805948779</td>\n",
       "      <td>59000.171844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3701</th>\n",
       "      <td>2204300054</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>130612472805722299</td>\n",
       "      <td>59000.171844</td>\n",
       "      <td>130612472805966979</td>\n",
       "      <td>59000.171844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4076</th>\n",
       "      <td>2204300629</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>130612472805767279</td>\n",
       "      <td>59000.171844</td>\n",
       "      <td>130612472805970569</td>\n",
       "      <td>59000.171844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4249</th>\n",
       "      <td>2204300469</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>130612472805712959</td>\n",
       "      <td>59000.171844</td>\n",
       "      <td>130612472805936069</td>\n",
       "      <td>59000.171844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4319</th>\n",
       "      <td>2204300365</td>\n",
       "      <td>67</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>130612472805720749</td>\n",
       "      <td>59000.171844</td>\n",
       "      <td>130612472805962359</td>\n",
       "      <td>59000.171844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4671</th>\n",
       "      <td>2204300139</td>\n",
       "      <td>54</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>130612472806300399</td>\n",
       "      <td>59000.171844</td>\n",
       "      <td>130612472806513969</td>\n",
       "      <td>59000.171844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4704</th>\n",
       "      <td>2204300423</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>130612472805776839</td>\n",
       "      <td>59000.171844</td>\n",
       "      <td>130612472805998679</td>\n",
       "      <td>59000.171844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4981</th>\n",
       "      <td>2204300562</td>\n",
       "      <td>73</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>130612472805718689</td>\n",
       "      <td>59000.171844</td>\n",
       "      <td>130612472805931779</td>\n",
       "      <td>59000.171844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5086</th>\n",
       "      <td>2204300541</td>\n",
       "      <td>66</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>130612473049845929</td>\n",
       "      <td>59000.171844</td>\n",
       "      <td>130612473050069829</td>\n",
       "      <td>59000.171844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5237</th>\n",
       "      <td>2204300205</td>\n",
       "      <td>69</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>130612472805726599</td>\n",
       "      <td>59000.171844</td>\n",
       "      <td>130612472805947649</td>\n",
       "      <td>59000.171844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5318</th>\n",
       "      <td>2204300138</td>\n",
       "      <td>80</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>130612472805730239</td>\n",
       "      <td>59000.171844</td>\n",
       "      <td>130612472805955969</td>\n",
       "      <td>59000.171844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6427</th>\n",
       "      <td>2204300545</td>\n",
       "      <td>85</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>130612472805894799</td>\n",
       "      <td>59000.171844</td>\n",
       "      <td>130612472806113109</td>\n",
       "      <td>59000.171844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6682</th>\n",
       "      <td>2204300641</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>130612472805710999</td>\n",
       "      <td>59000.171844</td>\n",
       "      <td>130612472805928849</td>\n",
       "      <td>59000.171844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6726</th>\n",
       "      <td>2204300671</td>\n",
       "      <td>81</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>130612472805763799</td>\n",
       "      <td>59000.171844</td>\n",
       "      <td>130612472805983039</td>\n",
       "      <td>59000.171844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7033</th>\n",
       "      <td>2204300130</td>\n",
       "      <td>37</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>130612472805819289</td>\n",
       "      <td>59000.171844</td>\n",
       "      <td>130612472806095649</td>\n",
       "      <td>59000.171844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7057</th>\n",
       "      <td>2204300021</td>\n",
       "      <td>79</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>130612472805715779</td>\n",
       "      <td>59000.171844</td>\n",
       "      <td>130612472805942349</td>\n",
       "      <td>59000.171844</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Run  Event  SubEvent  SubEventStream  exists  time_start_utc_daq  \\\n",
       "141   2204300670     89         0               0       1  130612472805802469   \n",
       "226   2204300612     99         0               0       1  130612472805775229   \n",
       "274   2204300304     72         0               0       1  130612472805729339   \n",
       "284   2204300574     13         0               0       1  130612472805725099   \n",
       "616   2204300282     57         0               0       1  130612472805790819   \n",
       "777   2204300667     83         0               0       1  130612472805704679   \n",
       "1147  2204300424     32         0               0       1  130612472805716559   \n",
       "1419  2204300455      1         0               0       1  130612472805719919   \n",
       "1441  2204300342     81         0               0       1  130612472805710669   \n",
       "1463  2204300535     67         0               0       1  130612472810244899   \n",
       "1713  2204300638     67         0               0       1  130612472956430029   \n",
       "1753  2204300078     29         0               0       1  130612472805797349   \n",
       "1783  2204300497     11         0               0       1  130612472805752779   \n",
       "2478  2204300415     97         0               0       1  130612472805811819   \n",
       "2760  2204300462     68         0               0       1  130612472805721549   \n",
       "2814  2204300646     18         0               0       1  130612472805721129   \n",
       "2825  2204300295     50         0               0       1  130612472805751749   \n",
       "3160  2204300653     66         0               0       1  130612472805733669   \n",
       "3319  2204300356     88         0               0       1  130612472805717329   \n",
       "3701  2204300054      4         0               0       1  130612472805722299   \n",
       "4076  2204300629      9         0               0       1  130612472805767279   \n",
       "4249  2204300469     35         0               0       1  130612472805712959   \n",
       "4319  2204300365     67         0               0       1  130612472805720749   \n",
       "4671  2204300139     54         0               0       1  130612472806300399   \n",
       "4704  2204300423     14         0               0       1  130612472805776839   \n",
       "4981  2204300562     73         0               0       1  130612472805718689   \n",
       "5086  2204300541     66         0               0       1  130612473049845929   \n",
       "5237  2204300205     69         0               0       1  130612472805726599   \n",
       "5318  2204300138     80         0               0       1  130612472805730239   \n",
       "6427  2204300545     85         0               0       1  130612472805894799   \n",
       "6682  2204300641     30         0               0       1  130612472805710999   \n",
       "6726  2204300671     81         0               0       1  130612472805763799   \n",
       "7033  2204300130     37         0               0       1  130612472805819289   \n",
       "7057  2204300021     79         0               0       1  130612472805715779   \n",
       "\n",
       "      time_start_mjd    time_end_utc_daq  time_end_mjd  \n",
       "141     59000.171844  130612472806023139  59000.171844  \n",
       "226     59000.171844  130612472806005409  59000.171844  \n",
       "274     59000.171844  130612472805933649  59000.171844  \n",
       "284     59000.171844  130612472805939449  59000.171844  \n",
       "616     59000.171844  130612472806004239  59000.171844  \n",
       "777     59000.171844  130612472805944849  59000.171844  \n",
       "1147    59000.171844  130612472805945729  59000.171844  \n",
       "1419    59000.171844  130612472805937009  59000.171844  \n",
       "1441    59000.171844  130612472805946249  59000.171844  \n",
       "1463    59000.171844  130612472810460059  59000.171844  \n",
       "1713    59000.171844  130612472956657199  59000.171844  \n",
       "1753    59000.171844  130612472806057329  59000.171844  \n",
       "1783    59000.171844  130612472805946559  59000.171844  \n",
       "2478    59000.171844  130612472806044339  59000.171844  \n",
       "2760    59000.171844  130612472805956889  59000.171844  \n",
       "2814    59000.171844  130612472805943029  59000.171844  \n",
       "2825    59000.171844  130612472805965189  59000.171844  \n",
       "3160    59000.171844  130612472805958429  59000.171844  \n",
       "3319    59000.171844  130612472805948779  59000.171844  \n",
       "3701    59000.171844  130612472805966979  59000.171844  \n",
       "4076    59000.171844  130612472805970569  59000.171844  \n",
       "4249    59000.171844  130612472805936069  59000.171844  \n",
       "4319    59000.171844  130612472805962359  59000.171844  \n",
       "4671    59000.171844  130612472806513969  59000.171844  \n",
       "4704    59000.171844  130612472805998679  59000.171844  \n",
       "4981    59000.171844  130612472805931779  59000.171844  \n",
       "5086    59000.171844  130612473050069829  59000.171844  \n",
       "5237    59000.171844  130612472805947649  59000.171844  \n",
       "5318    59000.171844  130612472805955969  59000.171844  \n",
       "6427    59000.171844  130612472806113109  59000.171844  \n",
       "6682    59000.171844  130612472805928849  59000.171844  \n",
       "6726    59000.171844  130612472805983039  59000.171844  \n",
       "7033    59000.171844  130612472806095649  59000.171844  \n",
       "7057    59000.171844  130612472805942349  59000.171844  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extract masked headers\n",
    "header_l3_masked = hdf_l3[\"I3EventHeader\"][mask_l3_neha][[\"Run\", \"Event\"]]\n",
    "header_reco_masked = hdf_reco[\"I3EventHeader\"][mask_reco_neha][[\"Run\", \"Event\"]]\n",
    "\n",
    "# Convert to sets of (Run, Event) pairs\n",
    "l3_events = set(zip(header_l3_masked[\"Run\"], header_l3_masked[\"Event\"]))\n",
    "reco_events = set(zip(header_reco_masked[\"Run\"], header_reco_masked[\"Event\"]))\n",
    "\n",
    "only_in_reco = reco_events - l3_events\n",
    "print(f\"Number of events only in reco after mask: {len(only_in_reco)}\")\n",
    "\n",
    "# Create boolean mask for those events\n",
    "reco_header = hdf_reco[\"I3EventHeader\"][mask_reco_neha]\n",
    "only_in_reco_mask = reco_header.apply(lambda row: (row[\"Run\"], row[\"Event\"]) in only_in_reco, axis=1)\n",
    "reco_only_events = reco_header[only_in_reco_mask]\n",
    "\n",
    "reco_only_events[reco_only_events[\"Event\"] < 100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have one event I will take a look at:\n",
    "1419  2204300455      1         0               0       1  130612472805719919   \n",
    "\n",
    "dataio-shovel /data/sim/IceCube/2020/filtered/level3/cascade/neutrino-generator/22043/0000000-0000999/Level3_NuMu_NuGenCCNC.022043.000455.i3.zst \n",
    "I dont see this event?\n",
    "\n",
    "dataio-shovel /data/ana/Diffuse/GlobalFit_Flavor/taupede/SnowStorm/RecowithBfr/Baseline/22043/0000000-0000999/Reco_NuMu_000455_out.i3.bz2 \n",
    "Here I find it:\n",
    "I3EventHeader [I3EventHeader]:\n",
    "[ I3EventHeader:\n",
    "        StartTime: 2020-05-31 04:07:27.280,571,991,9 UTC\n",
    "         EndTime : 2020-05-31 04:07:27.280,593,700,9 UTC\n",
    "           RunID : 2204300455\n",
    "        SubrunID : 4294967295\n",
    "         EventID : 1\n",
    "      SubEventID : 0\n",
    "  SubEventStream : InIceSplit\n",
    "]\n",
    "with CausalQTot 6839.98\n",
    "\n",
    "I do find it in the muon files.\n",
    "dataio-shovel /data/sim/IceCube/2020/filtered/level3/muon/neutrino-generator/22043/0000000-0000999/Level3_NuMu_NuGenCCNC.022043.000455.i3.zst \n",
    "\n",
    "I3EventHeader [I3EventHeader]:\n",
    "[ I3EventHeader:\n",
    "        StartTime: 2020-05-31 04:07:27.280,565,991,9 UTC\n",
    "         EndTime : 2020-05-31 04:07:27.280,597,699,9 UTC\n",
    "           RunID : 2204300455\n",
    "        SubrunID : 4294967295\n",
    "         EventID : 1\n",
    "      SubEventID : 0\n",
    "  SubEventStream : Final\n",
    "]\n",
    "\n",
    "Wait a second? Should I actually start with the cascade reco files?!?!?!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I should have not! I should start from the l2 files. I do that now from v3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import datasets\n",
    "import importlib\n",
    "\n",
    "importlib.reload(datasets)\n",
    "\n",
    "# set the inputs\n",
    "reco_versions = [\"spice_tau_reco\", \"v3\"]\n",
    "\n",
    "# Dynamically select the desired dataset\n",
    "simulation_datasets = {}\n",
    "for reco_version in reco_versions: simulation_datasets[reco_version] = getattr(datasets, reco_version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- Extracting files for NuTau_midE1\n",
      "----- Extracting files for NuTau_highE1\n",
      "----- Extracting files for NuTau_midE2\n",
      "----- Extracting files for NuTau_highE2\n",
      "----- Extracting files for NuE_midE1\n",
      "----- Extracting files for NuE_highE1\n",
      "----- Extracting files for NuE_midE2\n",
      "----- Extracting files for NuE_highE2\n",
      "----- Extracting files for NuMu_midE1\n",
      "----- Extracting files for NuMu_highE1\n",
      "----- Extracting files for NuMu_midE2\n",
      "----- Extracting files for NuMu_highE2\n",
      "----- Creating new key NuE\n",
      "Using NuE_midE1\n",
      "Using NuE_highE1\n",
      "Using NuE_midE2\n",
      "Using NuE_highE2\n",
      "----- Creating new key NuMu\n",
      "Using NuMu_midE1\n",
      "Using NuMu_highE1\n",
      "Using NuMu_midE2\n",
      "Using NuMu_highE2\n",
      "----- Creating new key NuTau\n",
      "Using NuTau_midE1\n",
      "Using NuTau_highE1\n",
      "Using NuTau_midE2\n",
      "Using NuTau_highE2\n",
      "----- Creating new key NuAll\n",
      "Using NuE\n",
      "Using NuMu\n",
      "Using NuTau\n",
      "----- Extracting files for NuTau_midE\n",
      "----- Extracting files for NuTau_highE\n",
      "----- Extracting files for NuE_midE\n",
      "----- Extracting files for NuE_highE\n",
      "----- Extracting files for NuMu_midE\n",
      "----- Extracting files for NuMu_highE\n",
      "----- Creating new key NuE\n",
      "Using NuE_midE\n",
      "Using NuE_highE\n",
      "----- Creating new key NuMu\n",
      "Using NuMu_midE\n",
      "Using NuMu_highE\n",
      "----- Creating new key NuTau\n",
      "Using NuTau_midE\n",
      "Using NuTau_highE\n",
      "----- Creating new key NuAll\n",
      "Using NuE\n",
      "Using NuMu\n",
      "Using NuTau\n"
     ]
    }
   ],
   "source": [
    "keys_to_merge[\"v3\"] = {\n",
    "    \"NuE\" : [\"NuE_midE\", \"NuE_highE\"],\n",
    "    \"NuMu\" : [\"NuMu_midE\", \"NuMu_highE\"],\n",
    "    \"NuTau\" : [\"NuTau_midE\", \"NuTau_highE\"],\n",
    "    \"NuAll\" : ['NuE', \"NuMu\", \"NuTau\"],\n",
    "}\n",
    "\n",
    "keys_to_merge[\"spice_tau_reco\"] = {\n",
    "    \"NuE\" : [\"NuE_midE1\", \"NuE_highE1\", \"NuE_midE2\", \"NuE_highE2\"],\n",
    "    \"NuMu\" : [\"NuMu_midE1\", \"NuMu_highE1\",\"NuMu_midE2\", \"NuMu_highE2\"],\n",
    "    \"NuTau\" : [\"NuTau_midE1\", \"NuTau_highE1\",\"NuTau_midE2\", \"NuTau_highE2\"],\n",
    "    \"NuAll\" : ['NuE', \"NuMu\", \"NuTau\"],\n",
    "\n",
    "}\n",
    "\n",
    "for key in simulation_datasets: simulation_datasets[key] = open_datasets( simulation_datasets[key], keys_to_merge[key] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   astro_NuE    astro_NuMu   astro_NuTau          conv        prompt\n",
      "spice_tau_reco  56.77 ± 0.56  20.42 ± 0.22  34.89 ± 0.43  38.77 ± 0.95  13.41 ± 0.11\n",
      "v3              56.81 ± 2.48  12.05 ± 0.88  34.89 ± 1.87  26.54 ± 3.13  12.24 ± 0.47\n"
     ]
    }
   ],
   "source": [
    "data = {}\n",
    "\n",
    "for key in simulation_datasets:\n",
    "\n",
    "    simulation_dataset = simulation_datasets[key]\n",
    "\n",
    "    channel_data = {}\n",
    "\n",
    "    for flavor in ['NuE', \"NuMu\", \"NuTau\"]:\n",
    "        weights = simulation_dataset[flavor][\"weighter\"].get_weights(AstroFluxModel) * livetime_s\n",
    "        rate = np.sum(weights)\n",
    "        error = np.sqrt(np.sum(weights**2))\n",
    "        channel_data[f\"astro_{flavor}\"] = f\"{rate:.2f} ± {error:.2f}\"\n",
    "\n",
    "    # Conventional\n",
    "    flavor = \"NuAll\"\n",
    "    weights_conv = simulation_dataset[flavor][\"weighter\"].get_weights(generator_conv) * livetime_s\n",
    "    rate_conv = np.sum(weights_conv)\n",
    "    err_conv = np.sqrt(np.sum(weights_conv**2))\n",
    "    channel_data[\"conv\"] = f\"{rate_conv:.2f} ± {err_conv:.2f}\"\n",
    "\n",
    "    # Prompt\n",
    "    weights_prompt = simulation_dataset[flavor][\"weighter\"].get_weights(generator_pr) * livetime_s\n",
    "    rate_prompt = np.sum(weights_prompt)\n",
    "    err_prompt = np.sqrt(np.sum(weights_prompt**2))\n",
    "    channel_data[\"prompt\"] = f\"{rate_prompt:.2f} ± {err_prompt:.2f}\"\n",
    "\n",
    "    data[key] = channel_data\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame.from_dict(data, orient='index')\n",
    "\n",
    "# Optional: specify column order\n",
    "columns_order = [f\"astro_{flavor}\" for flavor in ['NuE', 'NuMu', 'NuTau']] + [\"conv\", \"prompt\"]\n",
    "df = df[columns_order]\n",
    "\n",
    "# Display as string table\n",
    "print(df.to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am stilling missing muons!! Lets check\n",
    "- scratch log if I actually reconstructed level2 files: 22645 YES, 22644, YES\n",
    "- are there errors in the reco path: YES, some, but only the gulliver problem.\n",
    "Lets see if I actually have extra events with respect to v2, which was based on l3 cascade."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import datasets\n",
    "import importlib\n",
    "\n",
    "importlib.reload(datasets)\n",
    "\n",
    "# set the inputs\n",
    "reco_versions = [\"v2\", \"v3\"]\n",
    "\n",
    "# Dynamically select the desired dataset\n",
    "simulation_datasets = {}\n",
    "for reco_version in reco_versions: simulation_datasets[reco_version] = getattr(datasets, reco_version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- Extracting files for NuTau_midE\n",
      "----- Extracting files for NuTau_highE\n",
      "----- Extracting files for NuE_midE\n",
      "----- Extracting files for NuE_highE\n",
      "----- Extracting files for NuMu_midE\n",
      "----- Extracting files for NuMu_highE\n",
      "----- Creating new key NuE\n",
      "Using NuE_midE\n",
      "Using NuE_highE\n",
      "----- Creating new key NuMu\n",
      "Using NuMu_midE\n",
      "Using NuMu_highE\n",
      "----- Creating new key NuTau\n",
      "Using NuTau_midE\n",
      "Using NuTau_highE\n",
      "----- Creating new key NuAll\n",
      "Using NuE\n",
      "Using NuMu\n",
      "Using NuTau\n",
      "----- Extracting files for NuTau_midE\n",
      "----- Extracting files for NuTau_highE\n",
      "----- Extracting files for NuE_midE\n",
      "----- Extracting files for NuE_highE\n",
      "----- Extracting files for NuMu_midE\n",
      "----- Extracting files for NuMu_highE\n",
      "----- Creating new key NuE\n",
      "Using NuE_midE\n",
      "Using NuE_highE\n",
      "----- Creating new key NuMu\n",
      "Using NuMu_midE\n",
      "Using NuMu_highE\n",
      "----- Creating new key NuTau\n",
      "Using NuTau_midE\n",
      "Using NuTau_highE\n",
      "----- Creating new key NuAll\n",
      "Using NuE\n",
      "Using NuMu\n",
      "Using NuTau\n"
     ]
    }
   ],
   "source": [
    "keys_to_merge[\"v2\"] = {\n",
    "    \"NuE\" : [\"NuE_midE\", \"NuE_highE\"],\n",
    "    \"NuMu\" : [\"NuMu_midE\", \"NuMu_highE\"],\n",
    "    \"NuTau\" : [\"NuTau_midE\", \"NuTau_highE\"],\n",
    "    \"NuAll\" : ['NuE', \"NuMu\", \"NuTau\"],\n",
    "}\n",
    "keys_to_merge[\"v3\"] = {\n",
    "    \"NuE\" : [\"NuE_midE\", \"NuE_highE\"],\n",
    "    \"NuMu\" : [\"NuMu_midE\", \"NuMu_highE\"],\n",
    "    \"NuTau\" : [\"NuTau_midE\", \"NuTau_highE\"],\n",
    "    \"NuAll\" : ['NuE', \"NuMu\", \"NuTau\"],\n",
    "}\n",
    "for key in simulation_datasets: simulation_datasets[key] = open_datasets( simulation_datasets[key], keys_to_merge[key] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of events only in v2: 3\n",
      "{(2264500000, 1382), (2264500000, 3880), (2264500000, 47)}\n",
      "             Run  Event\n",
      "5442  2264500000     47\n",
      "5443  2264500000   1382\n",
      "5444  2264500000   3880\n",
      "Number of events only in v3: 0\n",
      "set()\n",
      "Empty DataFrame\n",
      "Columns: [Run, Event]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "file_path_v2 = \"/data/user/tvaneede/GlobalFit/reco_processing/hdf/output/v2/NuMu_22645_0000000-0000999.h5\"\n",
    "hdf_v2 = pd.HDFStore(file_path_v2,'r')\n",
    "\n",
    "file_path_v3 = \"/data/user/tvaneede/GlobalFit/reco_processing/hdf/output/v3/NuMu_22645_0000000-0000999.h5\"\n",
    "hdf_v3 = pd.HDFStore(file_path_v3,'r')\n",
    "\n",
    "run_mask_v2 = hdf_v2[\"I3EventHeader\"][\"Run\"] == 2264500000\n",
    "run_mask_v3 = hdf_v3[\"I3EventHeader\"][\"Run\"] == 2264500000\n",
    "\n",
    "# Extract masked headers\n",
    "header_v2_masked = hdf_v2[\"I3EventHeader\"][run_mask_v2][[\"Run\", \"Event\"]]\n",
    "header_v3_masked = hdf_v3[\"I3EventHeader\"][run_mask_v3][[\"Run\", \"Event\"]]\n",
    "\n",
    "# Convert to sets of (Run, Event) pairs\n",
    "v2_events = set(zip(header_v2_masked[\"Run\"], header_v2_masked[\"Event\"]))\n",
    "v3_events = set(zip(header_v3_masked[\"Run\"], header_v3_masked[\"Event\"]))\n",
    "\n",
    "only_in_v2 = v2_events - v3_events\n",
    "only_in_v3 = v3_events - v2_events\n",
    "print(f\"Number of events only in v2: {len(only_in_v2)}\")\n",
    "print(only_in_v2)\n",
    "print(header_v2_masked)\n",
    "print(f\"Number of events only in v3: {len(only_in_v3)}\")\n",
    "print(only_in_v3)\n",
    "print(header_v3_masked)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets check event 47:\n",
    "/data/sim/IceCube/2023/filtered/level2/neutrino-generator/22645/0000000-0000999/Level2_NuMu_NuGenCCNC.022645.000000.i3.zst\n",
    "Found event in frame 45, already has HESE_VHESelfVeto == false && HESE_CausalQTot == 22676.2\n",
    "\n",
    "Now in v2:\n",
    "/data/user/tvaneede/GlobalFit/reco_processing/output/v2/22645/0000000-0000999/Reco_NuMu_NuGenCCNC.022645.000000.i3.zst_out.i3.bz2\n",
    "I find the event in frame 7 with HESE_VHESelfVeto == false && HESE_CausalQTot == 22676.2.\n",
    "\n",
    "Now in v3:\n",
    "/data/user/tvaneede/GlobalFit/reco_processing/output/v3/22645/0000000-0000999/Reco_NuMu_NuGenCCNC.022645.000000.i3.zst_out.i3.bz2\n",
    "I also see the event!!\n",
    "\n",
    "Is there an error in the making of the hdf? NO\n",
    "Fuck.. I had a number with how many frames should be processed. I rerun the hdf maker, lets see:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- Extracting files for NuTau_midE1\n",
      "----- Extracting files for NuTau_highE1\n",
      "----- Extracting files for NuTau_midE2\n",
      "----- Extracting files for NuTau_highE2\n",
      "----- Extracting files for NuE_midE1\n",
      "----- Extracting files for NuE_highE1\n",
      "----- Extracting files for NuE_midE2\n",
      "----- Extracting files for NuE_highE2\n",
      "----- Extracting files for NuMu_midE1\n",
      "----- Extracting files for NuMu_highE1\n",
      "----- Extracting files for NuMu_midE2\n",
      "----- Extracting files for NuMu_highE2\n",
      "----- Creating new key NuE\n",
      "Using NuE_midE1\n",
      "Using NuE_highE1\n",
      "Using NuE_midE2\n",
      "Using NuE_highE2\n",
      "----- Creating new key NuMu\n",
      "Using NuMu_midE1\n",
      "Using NuMu_highE1\n",
      "Using NuMu_midE2\n",
      "Using NuMu_highE2\n",
      "----- Creating new key NuTau\n",
      "Using NuTau_midE1\n",
      "Using NuTau_highE1\n",
      "Using NuTau_midE2\n",
      "Using NuTau_highE2\n",
      "----- Creating new key NuAll\n",
      "Using NuE\n",
      "Using NuMu\n",
      "Using NuTau\n",
      "----- Extracting files for NuTau_midE\n",
      "----- Extracting files for NuTau_highE\n",
      "----- Extracting files for NuE_midE\n",
      "----- Extracting files for NuE_highE\n",
      "----- Extracting files for NuMu_midE\n",
      "----- Extracting files for NuMu_highE\n",
      "----- Creating new key NuE\n",
      "Using NuE_midE\n",
      "Using NuE_highE\n",
      "----- Creating new key NuMu\n",
      "Using NuMu_midE\n",
      "Using NuMu_highE\n",
      "----- Creating new key NuTau\n",
      "Using NuTau_midE\n",
      "Using NuTau_highE\n",
      "----- Creating new key NuAll\n",
      "Using NuE\n",
      "Using NuMu\n",
      "Using NuTau\n"
     ]
    }
   ],
   "source": [
    "from datasets import datasets\n",
    "import importlib\n",
    "\n",
    "importlib.reload(datasets)\n",
    "\n",
    "# set the inputs\n",
    "reco_versions = [\"spice_tau_reco\", \"v3\"]\n",
    "\n",
    "# Dynamically select the desired dataset\n",
    "simulation_datasets = {}\n",
    "for reco_version in reco_versions: simulation_datasets[reco_version] = getattr(datasets, reco_version)\n",
    "\n",
    "keys_to_merge[\"v3\"] = {\n",
    "    \"NuE\" : [\"NuE_midE\", \"NuE_highE\"],\n",
    "    \"NuMu\" : [\"NuMu_midE\", \"NuMu_highE\"],\n",
    "    \"NuTau\" : [\"NuTau_midE\", \"NuTau_highE\"],\n",
    "    \"NuAll\" : ['NuE', \"NuMu\", \"NuTau\"],\n",
    "}\n",
    "\n",
    "keys_to_merge[\"spice_tau_reco\"] = {\n",
    "    \"NuE\" : [\"NuE_midE1\", \"NuE_highE1\", \"NuE_midE2\", \"NuE_highE2\"],\n",
    "    \"NuMu\" : [\"NuMu_midE1\", \"NuMu_highE1\",\"NuMu_midE2\", \"NuMu_highE2\"],\n",
    "    \"NuTau\" : [\"NuTau_midE1\", \"NuTau_highE1\",\"NuTau_midE2\", \"NuTau_highE2\"],\n",
    "    \"NuAll\" : ['NuE', \"NuMu\", \"NuTau\"],\n",
    "\n",
    "}\n",
    "\n",
    "for key in simulation_datasets: simulation_datasets[key] = open_datasets( simulation_datasets[key], keys_to_merge[key] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   astro_NuE    astro_NuMu   astro_NuTau          conv        prompt\n",
      "spice_tau_reco  56.77 ± 0.56  20.42 ± 0.22  34.89 ± 0.43  38.77 ± 0.95  13.41 ± 0.11\n",
      "v3              56.81 ± 2.48  21.97 ± 1.15  37.11 ± 1.92  41.04 ± 3.85  13.81 ± 0.49\n"
     ]
    }
   ],
   "source": [
    "data = {}\n",
    "\n",
    "for key in simulation_datasets:\n",
    "\n",
    "    simulation_dataset = simulation_datasets[key]\n",
    "\n",
    "    channel_data = {}\n",
    "\n",
    "    for flavor in ['NuE', \"NuMu\", \"NuTau\"]:\n",
    "        weights = simulation_dataset[flavor][\"weighter\"].get_weights(AstroFluxModel) * livetime_s\n",
    "        rate = np.sum(weights)\n",
    "        error = np.sqrt(np.sum(weights**2))\n",
    "        channel_data[f\"astro_{flavor}\"] = f\"{rate:.2f} ± {error:.2f}\"\n",
    "\n",
    "    # Conventional\n",
    "    flavor = \"NuAll\"\n",
    "    weights_conv = simulation_dataset[flavor][\"weighter\"].get_weights(generator_conv) * livetime_s\n",
    "    rate_conv = np.sum(weights_conv)\n",
    "    err_conv = np.sqrt(np.sum(weights_conv**2))\n",
    "    channel_data[\"conv\"] = f\"{rate_conv:.2f} ± {err_conv:.2f}\"\n",
    "\n",
    "    # Prompt\n",
    "    weights_prompt = simulation_dataset[flavor][\"weighter\"].get_weights(generator_pr) * livetime_s\n",
    "    rate_prompt = np.sum(weights_prompt)\n",
    "    err_prompt = np.sqrt(np.sum(weights_prompt**2))\n",
    "    channel_data[\"prompt\"] = f\"{rate_prompt:.2f} ± {err_prompt:.2f}\"\n",
    "\n",
    "    data[key] = channel_data\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame.from_dict(data, orient='index')\n",
    "\n",
    "# Optional: specify column order\n",
    "columns_order = [f\"astro_{flavor}\" for flavor in ['NuE', 'NuMu', 'NuTau']] + [\"conv\", \"prompt\"]\n",
    "df = df[columns_order]\n",
    "\n",
    "# Display as string table\n",
    "print(df.to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem solved!! I think the main differences now come from statistics."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3-v4.4.1_reco-v1.1.0",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
