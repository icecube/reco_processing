{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import simweights\n",
    "import pickle\n",
    "import os, sys\n",
    "import re\n",
    "import numpy as np\n",
    "import matplotlib as mat\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import matplotlib.colors as colors\n",
    "import matplotlib.gridspec as gridspec\n",
    "import pandas as pd\n",
    "import tables\n",
    "import h5py\n",
    "import math\n",
    "from scipy.stats import mstats\n",
    "import matplotlib as mpl\n",
    "import matplotlib.font_manager as font_manager\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(\"/data/user/tvaneede/GlobalFit/reco_processing/notebooks/weighting\")\n",
    "from weights import *\n",
    "from utils import *\n",
    "from selections import selection_mask\n",
    "from fonts import *\n",
    "from plot_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Append the custom module path\n",
    "sys.path.append(\"/data/user/tvaneede/GlobalFit/reco_processing\")\n",
    "\n",
    "# Import the datasets module\n",
    "from datasets import datasets\n",
    "\n",
    "# set the inputs\n",
    "reco_versions = [\"evtgen_v1_rec_v2\", \"spice_tau_reco\"]\n",
    "\n",
    "# Dynamically select the desired dataset\n",
    "simulation_datasets = {}\n",
    "for reco_version in reco_versions: simulation_datasets[reco_version] = getattr(datasets, reco_version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "livetime_yr = 11.687\n",
    "livetime_s  = livetime_yr * 365.25 * 24 * 3600 # 11.687 year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main_plotting_path = f\"/data/user/tvaneede/GlobalFit/reco_processing/notebooks/compare_spice_ftp/output\"\n",
    "os.system(f\"mkdir -p {main_plotting_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weight functions\n",
    "spline_file = '/data/ana/Diffuse/NNMFit/MCEq_splines/v1.2.1/MCEq_splines_PRI-Gaisser-H4a_INT-SIBYLL23c_allfluxes.pickle'\n",
    "\n",
    "# conventional            \n",
    "flux_keys_conv =  ['conv_antinumu','conv_numu','conv_antinue','conv_nue','conv_antinutau','conv_nutau']\n",
    "spline_object_conv = SplineHandler(spline_file, flux_keys_conv)\n",
    "conv_flux = spline_object_conv.return_weight\n",
    "generator_conv = lambda pdgid, energy, cos_zen: conv_flux(pdgid, energy, cos_zen)\n",
    "\n",
    "# prompt\n",
    "flux_keys_pr =  ['pr_antinumu','pr_numu','pr_antinue','pr_nue','pr_antinutau','pr_nutau']\n",
    "spline_object_pr = SplineHandler(spline_file, flux_keys_pr)\n",
    "pr_flux = spline_object_pr.return_weight\n",
    "generator_pr = lambda pdgid, energy, cos_zen: pr_flux(pdgid, energy, cos_zen)\n",
    "\n",
    "# astro\n",
    "gamma_astro = 2.87\n",
    "per_flavor_norm = 2.12\n",
    "def AstroFluxModel(pdgid, energy, cos_zen):\n",
    "    flux = 0.5*(per_flavor_norm*1e-18)*(energy/1e5)**-gamma_astro\n",
    "    return flux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_datasets( simulation_dataset, keys_to_merge ):\n",
    "\n",
    "    # open the files\n",
    "    for key in simulation_dataset:\n",
    "        print(f\"----- Extracting files for {key}\")\n",
    "        simulation_dataset[key]['hdf_file'] = pd.HDFStore(simulation_dataset[key]['hdf_file_path'],'r')\n",
    "        simulation_dataset[key]['weighter'] = simweights.NuGenWeighter( simulation_dataset[key]['hdf_file'] ,nfiles=simulation_dataset[key]['nfiles'])\n",
    "\n",
    "    # merging files\n",
    "    for new_key in keys_to_merge:\n",
    "        print(f\"----- Creating new key {new_key}\")\n",
    "        simulation_dataset[new_key] = {}\n",
    "        simulation_dataset[new_key]['variables'] = {}\n",
    "        simulation_dataset[new_key]['weighter'] = None\n",
    "\n",
    "        for key in keys_to_merge[new_key]:\n",
    "            \n",
    "            print(f\"Using {key}\")\n",
    "            # merge the weighters\n",
    "            if simulation_dataset[new_key]['weighter'] == None:\n",
    "                simulation_dataset[new_key]['weighter'] = simulation_dataset[key]['weighter']\n",
    "            else: simulation_dataset[new_key]['weighter'] += simulation_dataset[key]['weighter']\n",
    "\n",
    "    # calculate weights\n",
    "    for key in simulation_dataset:\n",
    "        simulation_dataset[key]['weights_astro'] = simulation_dataset[key][\"weighter\"].get_weights(AstroFluxModel) * livetime_s\n",
    "        simulation_dataset[key]['weights_conv'] = simulation_dataset[key][\"weighter\"].get_weights(generator_conv) * livetime_s\n",
    "        simulation_dataset[key]['weights_pr'] = simulation_dataset[key][\"weighter\"].get_weights(generator_pr) * livetime_s\n",
    "\n",
    "\n",
    "    return simulation_dataset\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys_to_merge = {}\n",
    "\n",
    "keys_to_merge[\"evtgen_v1_rec_v2\"] = {\n",
    "    \"NuE\" : [\"NuE_midE\", \"NuE_highE\"],\n",
    "    \"NuMu\" : [\"NuMu_midE\", \"NuMu_highE\"],\n",
    "    \"NuTau\" : [\"NuTau_midE\", \"NuTau_highE\"],\n",
    "    \"NuAll\" : ['NuE', \"NuMu\", \"NuTau\"],\n",
    "}\n",
    "\n",
    "keys_to_merge[\"v2\"] = {\n",
    "    \"NuE\" : [\"NuE_midE\", \"NuE_highE\"],\n",
    "    \"NuMu\" : [\"NuMu_midE\", \"NuMu_highE\"],\n",
    "    \"NuTau\" : [\"NuTau_midE\", \"NuTau_highE\"],\n",
    "    \"NuAll\" : ['NuE', \"NuMu\", \"NuTau\"],\n",
    "}\n",
    "\n",
    "keys_to_merge[\"spice_tau_reco\"] = {\n",
    "    \"NuE\" : [\"NuE_midE1\", \"NuE_highE1\", \"NuE_midE2\", \"NuE_highE2\"],\n",
    "    \"NuMu\" : [\"NuMu_midE1\", \"NuMu_highE1\",\"NuMu_midE2\", \"NuMu_highE2\"],\n",
    "    \"NuTau\" : [\"NuTau_midE1\", \"NuTau_highE1\",\"NuTau_midE2\", \"NuTau_highE2\"],\n",
    "    \"NuAll\" : ['NuE', \"NuMu\", \"NuTau\"],\n",
    "\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- Extracting files for NuTau_midE\n",
      "----- Extracting files for NuTau_highE\n",
      "----- Extracting files for NuE_midE\n",
      "----- Extracting files for NuE_highE\n",
      "----- Extracting files for NuMu_midE\n",
      "----- Extracting files for NuMu_highE\n",
      "----- Creating new key NuE\n",
      "Using NuE_midE\n",
      "Using NuE_highE\n",
      "----- Creating new key NuMu\n",
      "Using NuMu_midE\n",
      "Using NuMu_highE\n",
      "----- Creating new key NuTau\n",
      "Using NuTau_midE\n",
      "Using NuTau_highE\n",
      "----- Creating new key NuAll\n",
      "Using NuE\n",
      "Using NuMu\n",
      "Using NuTau\n",
      "----- Extracting files for NuTau_midE1\n",
      "----- Extracting files for NuTau_highE1\n",
      "----- Extracting files for NuTau_midE2\n",
      "----- Extracting files for NuTau_highE2\n",
      "----- Extracting files for NuE_midE1\n",
      "----- Extracting files for NuE_highE1\n",
      "----- Extracting files for NuE_midE2\n",
      "----- Extracting files for NuE_highE2\n",
      "----- Extracting files for NuMu_midE1\n",
      "----- Extracting files for NuMu_highE1\n",
      "----- Extracting files for NuMu_midE2\n",
      "----- Extracting files for NuMu_highE2\n",
      "----- Creating new key NuE\n",
      "Using NuE_midE1\n",
      "Using NuE_highE1\n",
      "Using NuE_midE2\n",
      "Using NuE_highE2\n",
      "----- Creating new key NuMu\n",
      "Using NuMu_midE1\n",
      "Using NuMu_highE1\n",
      "Using NuMu_midE2\n",
      "Using NuMu_highE2\n",
      "----- Creating new key NuTau\n",
      "Using NuTau_midE1\n",
      "Using NuTau_highE1\n",
      "Using NuTau_midE2\n",
      "Using NuTau_highE2\n",
      "----- Creating new key NuAll\n",
      "Using NuE\n",
      "Using NuMu\n",
      "Using NuTau\n"
     ]
    }
   ],
   "source": [
    "for key in simulation_datasets: simulation_datasets[key] = open_datasets( simulation_datasets[key], keys_to_merge[key] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     astro_NuE    astro_NuMu   astro_NuTau          conv        prompt\n",
      "evtgen_v1_rec_v2  56.20 ± 0.54  14.74 ± 0.22  33.91 ± 0.39  32.36 ± 0.97  12.47 ± 0.10\n",
      "spice_tau_reco    56.77 ± 0.56  20.42 ± 0.22  34.89 ± 0.43  38.77 ± 0.95  13.41 ± 0.11\n"
     ]
    }
   ],
   "source": [
    "data = {}\n",
    "\n",
    "for key in simulation_datasets:\n",
    "\n",
    "    simulation_dataset = simulation_datasets[key]\n",
    "\n",
    "    channel_data = {}\n",
    "\n",
    "    for flavor in ['NuE', \"NuMu\", \"NuTau\"]:\n",
    "        weights = simulation_dataset[flavor][\"weighter\"].get_weights(AstroFluxModel) * livetime_s\n",
    "        rate = np.sum(weights)\n",
    "        error = np.sqrt(np.sum(weights**2))\n",
    "        channel_data[f\"astro_{flavor}\"] = f\"{rate:.2f} ± {error:.2f}\"\n",
    "\n",
    "    # Conventional\n",
    "    flavor = \"NuAll\"\n",
    "    weights_conv = simulation_dataset[flavor][\"weighter\"].get_weights(generator_conv) * livetime_s\n",
    "    rate_conv = np.sum(weights_conv)\n",
    "    err_conv = np.sqrt(np.sum(weights_conv**2))\n",
    "    channel_data[\"conv\"] = f\"{rate_conv:.2f} ± {err_conv:.2f}\"\n",
    "\n",
    "    # Prompt\n",
    "    weights_prompt = simulation_dataset[flavor][\"weighter\"].get_weights(generator_pr) * livetime_s\n",
    "    rate_prompt = np.sum(weights_prompt)\n",
    "    err_prompt = np.sqrt(np.sum(weights_prompt**2))\n",
    "    channel_data[\"prompt\"] = f\"{rate_prompt:.2f} ± {err_prompt:.2f}\"\n",
    "\n",
    "    data[key] = channel_data\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame.from_dict(data, orient='index')\n",
    "\n",
    "# Optional: specify column order\n",
    "columns_order = [f\"astro_{flavor}\" for flavor in ['NuE', 'NuMu', 'NuTau']] + [\"conv\", \"prompt\"]\n",
    "df = df[columns_order]\n",
    "\n",
    "# Display as string table\n",
    "print(df.to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 astro_NuMu_midE astro_NuMu_highE\n",
      "evtgen_v1_rec_v2    13.92 ± 0.21      0.82 ± 0.04\n",
      "               astro_NuMu_midE1 astro_NuMu_highE1 astro_NuMu_midE2 astro_NuMu_highE2\n",
      "spice_tau_reco     19.28 ± 0.41       1.22 ± 0.04     19.18 ± 0.27       1.21 ± 0.03\n"
     ]
    }
   ],
   "source": [
    "for key in simulation_datasets:\n",
    "\n",
    "    data = {}\n",
    "    simulation_dataset = simulation_datasets[key]\n",
    "\n",
    "    channel_data = {}\n",
    "\n",
    "    for flavor in keys_to_merge[key][\"NuMu\"]:\n",
    "        weights = simulation_dataset[flavor][\"weighter\"].get_weights(AstroFluxModel) * livetime_s\n",
    "        rate = np.sum(weights)\n",
    "        error = np.sqrt(np.sum(weights**2))\n",
    "        channel_data[f\"astro_{flavor}\"] = f\"{rate:.2f} ± {error:.2f}\"\n",
    "\n",
    "    data[key] = channel_data\n",
    "\n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame.from_dict(data, orient='index')\n",
    "\n",
    "    # Display as string table\n",
    "    print(df.to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I seem to have only 72-67%. Let's see if v2 of the reco is fine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import datasets\n",
    "import importlib\n",
    "\n",
    "importlib.reload(datasets)\n",
    "\n",
    "# set the inputs\n",
    "reco_versions = [\"v2\", \"spice_tau_reco\"]\n",
    "\n",
    "# Dynamically select the desired dataset\n",
    "simulation_datasets = {}\n",
    "for reco_version in reco_versions: simulation_datasets[reco_version] = getattr(datasets, reco_version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- Extracting files for NuTau_midE\n",
      "----- Extracting files for NuTau_highE\n",
      "----- Extracting files for NuE_midE\n",
      "----- Extracting files for NuE_highE\n",
      "----- Extracting files for NuMu_midE\n",
      "----- Extracting files for NuMu_highE\n",
      "----- Creating new key NuE\n",
      "Using NuE_midE\n",
      "Using NuE_highE\n",
      "----- Creating new key NuMu\n",
      "Using NuMu_midE\n",
      "Using NuMu_highE\n",
      "----- Creating new key NuTau\n",
      "Using NuTau_midE\n",
      "Using NuTau_highE\n",
      "----- Creating new key NuAll\n",
      "Using NuE\n",
      "Using NuMu\n",
      "Using NuTau\n",
      "----- Extracting files for NuTau_midE1\n",
      "----- Extracting files for NuTau_highE1\n",
      "----- Extracting files for NuTau_midE2\n",
      "----- Extracting files for NuTau_highE2\n",
      "----- Extracting files for NuE_midE1\n",
      "----- Extracting files for NuE_highE1\n",
      "----- Extracting files for NuE_midE2\n",
      "----- Extracting files for NuE_highE2\n",
      "----- Extracting files for NuMu_midE1\n",
      "----- Extracting files for NuMu_highE1\n",
      "----- Extracting files for NuMu_midE2\n",
      "----- Extracting files for NuMu_highE2\n",
      "----- Creating new key NuE\n",
      "Using NuE_midE1\n",
      "Using NuE_highE1\n",
      "Using NuE_midE2\n",
      "Using NuE_highE2\n",
      "----- Creating new key NuMu\n",
      "Using NuMu_midE1\n",
      "Using NuMu_highE1\n",
      "Using NuMu_midE2\n",
      "Using NuMu_highE2\n",
      "----- Creating new key NuTau\n",
      "Using NuTau_midE1\n",
      "Using NuTau_highE1\n",
      "Using NuTau_midE2\n",
      "Using NuTau_highE2\n",
      "----- Creating new key NuAll\n",
      "Using NuE\n",
      "Using NuMu\n",
      "Using NuTau\n"
     ]
    }
   ],
   "source": [
    "for key in simulation_datasets: simulation_datasets[key] = open_datasets( simulation_datasets[key], keys_to_merge[key] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   astro_NuMu_midE astro_NuMu_highE\n",
      "v2    13.92 ± 0.21      0.82 ± 0.04\n",
      "               astro_NuMu_midE1 astro_NuMu_highE1 astro_NuMu_midE2 astro_NuMu_highE2\n",
      "spice_tau_reco     19.28 ± 0.41       1.22 ± 0.04     19.18 ± 0.27       1.21 ± 0.03\n"
     ]
    }
   ],
   "source": [
    "for key in simulation_datasets:\n",
    "\n",
    "    data = {}\n",
    "    simulation_dataset = simulation_datasets[key]\n",
    "\n",
    "    channel_data = {}\n",
    "\n",
    "    for flavor in keys_to_merge[key][\"NuMu\"]:\n",
    "        weights = simulation_dataset[flavor][\"weighter\"].get_weights(AstroFluxModel) * livetime_s\n",
    "        rate = np.sum(weights)\n",
    "        error = np.sqrt(np.sum(weights**2))\n",
    "        channel_data[f\"astro_{flavor}\"] = f\"{rate:.2f} ± {error:.2f}\"\n",
    "\n",
    "    data[key] = channel_data\n",
    "\n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame.from_dict(data, orient='index')\n",
    "\n",
    "    # Display as string table\n",
    "    print(df.to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also missing! Let's take a look at ftp_l3casc and do a cut myself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import datasets\n",
    "import importlib\n",
    "\n",
    "importlib.reload(datasets)\n",
    "\n",
    "# set the inputs\n",
    "reco_versions = [\"ftp_l3casc\", \"spice_l3casc\"]\n",
    "\n",
    "# Dynamically select the desired dataset\n",
    "simulation_datasets = {}\n",
    "for reco_version in reco_versions: simulation_datasets[reco_version] = getattr(datasets, reco_version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- Extracting files for NuTau_midE1\n",
      "----- Extracting files for NuTau_midE2\n",
      "----- Extracting files for NuTau_highE1\n",
      "----- Extracting files for NuTau_highE2\n",
      "----- Extracting files for NuE_midE1\n",
      "----- Extracting files for NuE_midE2\n",
      "----- Extracting files for NuE_highE1\n",
      "----- Extracting files for NuE_highE2\n",
      "----- Extracting files for NuMu_midE1\n",
      "----- Extracting files for NuMu_midE2\n",
      "----- Extracting files for NuMu_highE1\n",
      "----- Extracting files for NuMu_highE2\n",
      "----- Creating new key NuE\n",
      "Using NuE_midE1\n",
      "Using NuE_highE1\n",
      "Using NuE_midE2\n",
      "Using NuE_highE2\n",
      "----- Creating new key NuMu\n",
      "Using NuMu_midE1\n",
      "Using NuMu_highE1\n",
      "Using NuMu_midE2\n",
      "Using NuMu_highE2\n",
      "----- Creating new key NuTau\n",
      "Using NuTau_midE1\n",
      "Using NuTau_highE1\n",
      "Using NuTau_midE2\n",
      "Using NuTau_highE2\n",
      "----- Creating new key NuAll\n",
      "Using NuE\n",
      "Using NuMu\n",
      "Using NuTau\n",
      "----- Extracting files for NuTau_midE1\n",
      "----- Extracting files for NuTau_highE1\n",
      "----- Extracting files for NuTau_midE2\n",
      "----- Extracting files for NuTau_highE2\n",
      "----- Extracting files for NuE_midE1\n",
      "----- Extracting files for NuE_highE1\n",
      "----- Extracting files for NuE_midE2\n",
      "----- Extracting files for NuE_highE2\n",
      "----- Extracting files for NuMu_midE1\n",
      "----- Extracting files for NuMu_highE1\n",
      "----- Extracting files for NuMu_midE2\n",
      "----- Extracting files for NuMu_highE2\n",
      "----- Creating new key NuE\n",
      "Using NuE_midE1\n",
      "Using NuE_highE1\n",
      "Using NuE_midE2\n",
      "Using NuE_highE2\n",
      "----- Creating new key NuMu\n",
      "Using NuMu_midE1\n",
      "Using NuMu_highE1\n",
      "Using NuMu_midE2\n",
      "Using NuMu_highE2\n",
      "----- Creating new key NuTau\n",
      "Using NuTau_midE1\n",
      "Using NuTau_highE1\n",
      "Using NuTau_midE2\n",
      "Using NuTau_highE2\n",
      "----- Creating new key NuAll\n",
      "Using NuE\n",
      "Using NuMu\n",
      "Using NuTau\n"
     ]
    }
   ],
   "source": [
    "keys_to_merge[\"ftp_l3casc\"] = {\n",
    "    \"NuE\" : [\"NuE_midE1\", \"NuE_highE1\", \"NuE_midE2\", \"NuE_highE2\"],\n",
    "    \"NuMu\" : [\"NuMu_midE1\", \"NuMu_highE1\",\"NuMu_midE2\", \"NuMu_highE2\"],\n",
    "    \"NuTau\" : [\"NuTau_midE1\", \"NuTau_highE1\",\"NuTau_midE2\", \"NuTau_highE2\"],\n",
    "    \"NuAll\" : ['NuE', \"NuMu\", \"NuTau\"],   \n",
    "}\n",
    "\n",
    "keys_to_merge[\"spice_l3casc\"] = {\n",
    "    \"NuE\" : [\"NuE_midE1\", \"NuE_highE1\", \"NuE_midE2\", \"NuE_highE2\"],\n",
    "    \"NuMu\" : [\"NuMu_midE1\", \"NuMu_highE1\",\"NuMu_midE2\", \"NuMu_highE2\"],\n",
    "    \"NuTau\" : [\"NuTau_midE1\", \"NuTau_highE1\",\"NuTau_midE2\", \"NuTau_highE2\"],\n",
    "    \"NuAll\" : ['NuE', \"NuMu\", \"NuTau\"],\n",
    "}\n",
    "\n",
    "for key in simulation_datasets: simulation_datasets[key] = open_datasets( simulation_datasets[key], keys_to_merge[key] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  astro_NuMu_midE1 astro_NuMu_highE1 astro_NuMu_midE2 astro_NuMu_highE2\n",
      "ftp_l3casc           890.37 ± 1.65       8.84 ± 0.05    894.52 ± 1.70       8.86 ± 0.06\n",
      "ftp_l3casc_masked     13.94 ± 0.13       0.83 ± 0.01     14.18 ± 0.14       0.84 ± 0.02\n"
     ]
    }
   ],
   "source": [
    "data = {}\n",
    "\n",
    "key = \"ftp_l3casc\"\n",
    "\n",
    "simulation_dataset = simulation_datasets[key]\n",
    "\n",
    "channel_data = {}\n",
    "channel_data_masked = {}\n",
    "\n",
    "for flavor in keys_to_merge[key][\"NuMu\"]:\n",
    "    weights = simulation_dataset[flavor][\"weighter\"].get_weights(AstroFluxModel) * livetime_s\n",
    "    rate = np.sum(weights)\n",
    "    error = np.sqrt(np.sum(weights**2))\n",
    "    channel_data[f\"astro_{flavor}\"] = f\"{rate:.2f} ± {error:.2f}\"\n",
    "\n",
    "    HESE_CausalQTot = simulation_dataset[flavor][\"hdf_file\"][\"HESE_CausalQTot\"].value\n",
    "    mask = HESE_CausalQTot > 6000\n",
    "    rate_masked = np.sum(weights[mask])\n",
    "    error_masked = np.sqrt(np.sum(weights[mask]**2))\n",
    "    channel_data_masked[f\"astro_{flavor}\"] = f\"{rate_masked:.2f} ± {error_masked:.2f}\"\n",
    "\n",
    "data[key] = channel_data\n",
    "data[f\"{key}_masked\"] = channel_data_masked\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame.from_dict(data, orient='index')\n",
    "\n",
    "# Display as string table\n",
    "print(df.to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seems to be the same amount of events missing. Lets check one dataset of 0000000-0000999"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rate_NuMu_midE 888.3088102238869 14.876935899724039\n",
      "rate_NuMu_highE 8.933702850310775 0.9000591852292844\n"
     ]
    }
   ],
   "source": [
    "file_path_NuMu_midE = \"/data/user/tvaneede/GlobalFit/reco_processing/hdf/output/ftp_l3casc/NuMu_22645_0000000-0000999.h5\"\n",
    "hdf_NuMu_midE = pd.HDFStore(file_path_NuMu_midE,'r')\n",
    "nfiles_NuMu_midE = 1000\n",
    "weighter_NuMu_midE = simweights.NuGenWeighter( hdf_NuMu_midE, nfiles=nfiles_NuMu_midE)\n",
    "weights_NuMu_midE = weighter_NuMu_midE.get_weights(AstroFluxModel) * livetime_s\n",
    "rate_NuMu_midE = np.sum(weights_NuMu_midE)\n",
    "mask_NuMu_midE = hdf_NuMu_midE[\"HESE_CausalQTot\"].value > 6000\n",
    "rate_masked_NuMu_midE = np.sum(weights_NuMu_midE[mask_NuMu_midE])\n",
    "\n",
    "print(\"rate_NuMu_midE\", rate_NuMu_midE, rate_masked_NuMu_midE)\n",
    "\n",
    "file_path_NuMu_highE = \"/data/user/tvaneede/GlobalFit/reco_processing/hdf/output/ftp_l3casc/NuMu_22644_0000000-0000999.h5\"\n",
    "hdf_NuMu_highE = pd.HDFStore(file_path_NuMu_highE,'r')\n",
    "nfiles_NuMu_highE = 1000\n",
    "weighter_NuMu_highE = simweights.NuGenWeighter( hdf_NuMu_highE, nfiles=nfiles_NuMu_highE)\n",
    "weights_NuMu_highE = weighter_NuMu_highE.get_weights(AstroFluxModel) * livetime_s\n",
    "rate_NuMu_highE = np.sum(weights_NuMu_highE)\n",
    "mask_NuMu_highE = hdf_NuMu_highE[\"HESE_CausalQTot\"].value > 6000\n",
    "rate_masked_NuMu_highE = np.sum(weights_NuMu_highE[mask_NuMu_highE])\n",
    "\n",
    "print(\"rate_NuMu_highE\", rate_NuMu_highE, rate_masked_NuMu_highE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am really starting to believe that we actually have fewer muon neutrinos at hese level for the ftp-v3 simulations. Lets make a hdf of the spice files at cascade level to see if there is a difference there as well?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Turns out one reco file was corrupted, so the hdf of that group of files was broken, see tools/find_error_in_log.py\n",
    "\n",
    "Missing jobs: 0\n",
    "\n",
    "Error jobs: 1\n",
    "{'NuMu_22043_0000000-0000999': {'LOGDIR': '/scratch/tvaneede/reco/hdf_taupede_tianlu/spice_l3casc/hdf_dag_spice_l3casc/logs', 'JOBID': 'NuMu_22043_0000000-0000999', 'INPATH': '/data/sim/IceCube/2020/filtered/level3/cascade/neutrino-generator/22043/0000000-0000999', 'OUTFILE': '/data/user/tvaneede/GlobalFit/reco_processing/hdf/output/spice_l3casc/NuMu_22043_0000000-0000999.h5'}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88234 88234 83872 83872\n",
      "Number of missing events: 4362\n"
     ]
    }
   ],
   "source": [
    "key = \"spice_l3casc\"\n",
    "flavor = \"NuMu_midE1\"\n",
    "weights = simulation_dataset[flavor][\"weighter\"].get_weights(AstroFluxModel) * livetime_s\n",
    "HESE_CausalQTot = simulation_dataset[flavor][\"weighter\"].get_column(\"HESE_CausalQTot\", \"value\")\n",
    "HESE_CausalQTot_2 = simulation_dataset[flavor][\"hdf_file\"][\"HESE_CausalQTot\"]\n",
    "I3MCWeightDict = simulation_dataset[flavor][\"hdf_file\"][\"I3MCWeightDict\"]\n",
    "\n",
    "print(len(weights),len(I3MCWeightDict),len(HESE_CausalQTot), len(HESE_CausalQTot_2))\n",
    "# mask = simulation_dataset[flavor][\"weighter\"].get_selection_mask()\n",
    "\n",
    "# Extract identifiers from both tables\n",
    "mc = simulation_dataset[flavor][\"hdf_file\"][\"I3MCWeightDict\"]\n",
    "qtot = simulation_dataset[flavor][\"hdf_file\"][\"HESE_CausalQTot\"]\n",
    "\n",
    "# Convert to DataFrames using common keys\n",
    "id_keys = [\"Run\", \"Event\", \"SubEvent\"]\n",
    "\n",
    "df_mc = pd.DataFrame({key: mc[key] for key in id_keys})\n",
    "df_qtot = pd.DataFrame({key: qtot[key] for key in id_keys})\n",
    "\n",
    "# Merge with indicator to find which are missing from qtot\n",
    "merged = df_mc.merge(df_qtot, on=id_keys, how='left', indicator=True)\n",
    "\n",
    "# Rows in I3MCWeightDict that are missing from HESE_CausalQTot\n",
    "missing = merged[merged[\"_merge\"] == \"left_only\"]\n",
    "\n",
    "print(f\"Number of missing events: {len(missing)}\")\n",
    "# print(missing[missing[\"Event\"] < 100])\n",
    "\n",
    "# we have a problem in creating the hdf file of \n",
    "# /data/sim/IceCube/2020/filtered/level3/cascade/neutrino-generator/22043/0000000-0000999/Level3_NuMu_NuGenCCNC.022043.000196.i3.zst\n",
    "# hdf_test = pd.HDFStore(\"/data/sim/IceCube/2020/filtered/level3/cascade/neutrino-generator/22043/0000000-0000999/Level3_NuMu_NuGenCCNC.022043.000196.i3.zst\",'r')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    astro_NuMu_highE1 astro_NuMu_midE2 astro_NuMu_highE2\n",
      "spice_l3casc              9.31 ± 0.13    919.78 ± 2.97       9.19 ± 0.08\n",
      "spice_l3casc_masked       0.81 ± 0.04     13.77 ± 0.23       0.82 ± 0.02\n"
     ]
    }
   ],
   "source": [
    "data = {}\n",
    "\n",
    "key = \"spice_l3casc\"\n",
    "\n",
    "simulation_dataset = simulation_datasets[key]\n",
    "\n",
    "channel_data = {}\n",
    "channel_data_masked = {}\n",
    "\n",
    "for flavor in [\"NuMu_highE1\",\"NuMu_midE2\", \"NuMu_highE2\"]: # \"NuMu_midE1\" was/is corrupt\n",
    "    weights = simulation_dataset[flavor][\"weighter\"].get_weights(AstroFluxModel) * livetime_s\n",
    "    rate = np.sum(weights)\n",
    "    error = np.sqrt(np.sum(weights**2))\n",
    "    channel_data[f\"astro_{flavor}\"] = f\"{rate:.2f} ± {error:.2f}\"\n",
    "\n",
    "    HESE_CausalQTot = simulation_dataset[flavor][\"hdf_file\"][\"HESE_CausalQTot\"].value\n",
    "    mask = HESE_CausalQTot > 6000\n",
    "    rate_masked = np.sum(weights[mask])\n",
    "    error_masked = np.sqrt(np.sum(weights[mask]**2))\n",
    "    channel_data_masked[f\"astro_{flavor}\"] = f\"{rate_masked:.2f} ± {error_masked:.2f}\"\n",
    "\n",
    "data[key] = channel_data\n",
    "data[f\"{key}_masked\"] = channel_data_masked\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame.from_dict(data, orient='index')\n",
    "\n",
    "# Display as string table\n",
    "print(df.to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow!! It turns out, if I do the cut myself on spice, I get the same number. How did Neha get higher values? Probably due to her definitions in \n",
    "https://github.com/icecube/wg-diffuse/blob/2023_GlobalFit_Flavor/Ternary_Classifier/segments/VHESelfVeto.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3-v4.4.1_reco-v1.1.0",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
