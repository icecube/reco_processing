{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import matplotlib.pyplot as plt\n",
    "import h5py\n",
    "import pandas as pd\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets_tau_reco = {\n",
    "  23436: {\"flavor\": \"NuMu\", \"energy\": \"low\", \"true_dataset\": 22646, \"nfiles\" : 8000},\n",
    "  23435: {\"flavor\": \"NuMu\", \"energy\": \"mid\", \"true_dataset\": 22645, \"nfiles\" : 5000},\n",
    "  23434: {\"flavor\": \"NuMu\", \"energy\": \"high\", \"true_dataset\": 22644, \"nfiles\" : 15000},\n",
    "  23433: {\"flavor\": \"NuTau\", \"energy\": \"high\", \"true_dataset\": 22635, \"nfiles\" : 20000},\n",
    "  23432: {\"flavor\": \"NuTau\", \"energy\": \"mid\", \"true_dataset\": 22634, \"nfiles\" : 4000},\n",
    "  23431: {\"flavor\": \"NuTau\", \"energy\": \"low\", \"true_dataset\": 22633, \"nfiles\" : 1000},\n",
    "  23430: {\"flavor\": \"NuE\", \"energy\": \"low\", \"true_dataset\": 22614, \"nfiles\" : 1000},\n",
    "  23429: {\"flavor\": \"NuE\", \"energy\": \"mid\", \"true_dataset\": 22613, \"nfiles\" : 4000},\n",
    "  23428: {\"flavor\": \"NuE\", \"energy\": \"high\", \"true_dataset\": 22612, \"nfiles\" : 20000}\n",
    "}\n",
    "\n",
    "datasets_level4_6 = {\n",
    "  23155: {\"flavor\": \"NuMu\", \"energy\": \"low\", \"true_dataset\": 22646},\n",
    "  23154: {\"flavor\": \"NuMu\", \"energy\": \"mid\", \"true_dataset\": 22645},\n",
    "  23153: {\"flavor\": \"NuMu\", \"energy\": \"high\", \"true_dataset\": 22644},\n",
    "  23152: {\"flavor\": \"NuTau\", \"energy\": \"high\", \"true_dataset\": 22635},\n",
    "  23151: {\"flavor\": \"NuTau\", \"energy\": \"mid\", \"true_dataset\": 22634},\n",
    "  23150: {\"flavor\": \"NuTau\", \"energy\": \"low\", \"true_dataset\": 22633},\n",
    "  23149: {\"flavor\": \"NuE\", \"energy\": \"low\", \"true_dataset\": 22614},\n",
    "  23148: {\"flavor\": \"NuE\", \"energy\": \"mid\", \"true_dataset\": 22613},\n",
    "  23147: {\"flavor\": \"NuE\", \"energy\": \"high\", \"true_dataset\": 22612}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open datasets\n",
    "for dataset_id in datasets_tau_reco:\n",
    "    datasets_tau_reco[dataset_id][\"df\"] = df = pd.read_hdf(f\"/data/user/tvaneede/GlobalFit/reco_processing/notebooks/benchmark_tau_reco_iceprod/requirements/data/{dataset_id}.hdf5\", key=f'/{dataset_id}')\n",
    "\n",
    "for dataset_id in datasets_level4_6:\n",
    "    datasets_level4_6[dataset_id][\"df\"] = df = pd.read_hdf(f\"/data/user/tvaneede/GlobalFit/SnowStorm_systematics/iceprod_req_harvest/data/{dataset_id}.hdf5\", key=f'/{dataset_id}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_dict = {\n",
    "    \"low\" : {\n",
    "        0 : \"Level7_Cascade_cascade\",\n",
    "        1 : \"Level8_Cascade_cascade\",\n",
    "    },\n",
    "    \"mid_high\" : {\n",
    "        0 : \"Filter_HESE+Taupede\",\n",
    "        1 : \"EvtGen_HESE\",\n",
    "        2 : \"Level7_Cascade_cascade\",\n",
    "        3 : \"Level8_Cascade_cascade\",\n",
    "    },\n",
    "    \"level4_6\" : {\n",
    "        0 : \"Level4_Cascade\",\n",
    "        1 : \"Level5_Cascade_cascade\",\n",
    "        2 : \"Level5_Cascade_muon\",\n",
    "        3 : \"Level5_Cascade_hybrid\",\n",
    "        4 : \"Level6_Cascade_cascade\",\n",
    "        5 : \"Level6_Cascade_muon\",\n",
    "        6 : \"Level6_Cascade_hybrid\",\n",
    "    }\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_mean_usage( variable, dataset_id, df ):\n",
    "\n",
    "    # Find the number of tasks in this dataset\n",
    "    tasks = df.index.get_level_values('task').unique()\n",
    "    tasks = tasks[tasks >= 0]  # just in case\n",
    "\n",
    "    result = {\"total\" : 0}\n",
    "\n",
    "    for i, task in enumerate(tasks):\n",
    "        # Mask for this dataset and task\n",
    "        mask = (df.index.get_level_values('dataset') == dataset_id) & \\\n",
    "            (df.index.get_level_values('task') == task)\n",
    "        df_mask = df[mask]\n",
    "        mean = df_mask[variable].mean()\n",
    "        result[i] = mean; result[\"total\"] += mean\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obtain_file_size( dataset_id ):\n",
    "\n",
    "    true_id = datasets_tau_reco[dataset_id][\"true_dataset\"]\n",
    "\n",
    "    file_paths = [f\"/data/sim/IceCube/2023/filtered/level7/cascade/neutrino-generator/cascade/{true_id}/0000000-0000999/\",\n",
    "                  f\"/data/sim/IceCube/2023/filtered/level8/cascade/neutrino-generator/cascade/{true_id}/0000000-0000999/\"]\n",
    "\n",
    "    if \"low\" not in datasets_tau_reco[dataset_id][\"energy\"]:\n",
    "        file_paths += [f\"/data/sim/IceCube/2023/filtered/HESE/neutrino-generator/taupede/{true_id}/0000000-0000999/\",\n",
    "                            f\"/data/sim/IceCube/2023/filtered/HESE/neutrino-generator/evtgen/{true_id}/0000000-0000999/\" ]\n",
    "\n",
    "    result = {\"total\" : 0}\n",
    "\n",
    "    for i,file_path in enumerate(file_paths):\n",
    "\n",
    "        sizes = []\n",
    "        for fname in os.listdir(file_path):\n",
    "            sizes.append(os.path.getsize(os.path.join(file_path, fname)))\n",
    "\n",
    "        avg_size = np.mean(sizes) / 1e9  # in GB\n",
    "        result[i] = avg_size\n",
    "\n",
    "        result[\"total\"] +=avg_size\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  dataset        type  nfiles  cpu_hours  Space (GB)\n",
      "0   22646    NuMu_low    8000      13397         164\n",
      "1   22645    NuMu_mid    5000      13834         218\n",
      "2   22644   NuMu_high   15000      23149         460\n",
      "3   22635  NuTau_high   20000      36780         991\n",
      "4   22634   NuTau_mid    4000      17046         369\n",
      "5   22633   NuTau_low    1000       8579          79\n",
      "6   22614     NuE_low    1000      17480         182\n",
      "7   22613     NuE_mid    4000      31287         667\n",
      "8   22612    NuE_high   20000      36572         965\n",
      "9   Total               78000     198124        4095\n"
     ]
    }
   ],
   "source": [
    "# Collect rows in a list\n",
    "rows = []\n",
    "for dataset_id, info in datasets_tau_reco.items():\n",
    "\n",
    "    df = datasets_tau_reco[dataset_id][\"df\"]\n",
    "\n",
    "    mean_cpu_hours = extract_mean_usage( \"time_used\",dataset_id, df )[\"total\"]\n",
    "    average_total_file_size = obtain_file_size( dataset_id )[\"total\"]\n",
    "\n",
    "    row = {\n",
    "        \"dataset\": info[\"true_dataset\"],\n",
    "        \"type\": f'{info[\"flavor\"]}_{info[\"energy\"]}',\n",
    "        \"nfiles\": info[\"nfiles\"],\n",
    "        \"cpu_hours\": int(info[\"nfiles\"]*mean_cpu_hours),\n",
    "        \"Space (GB)\": int(info[\"nfiles\"]*average_total_file_size),\n",
    "    }\n",
    "    rows.append(row)\n",
    "\n",
    "# Convert list of dicts → DataFrame\n",
    "df = pd.DataFrame(rows)\n",
    "\n",
    "# Add a final row with sums\n",
    "sum_row = {\n",
    "    \"dataset\": \"Total\",\n",
    "    \"type\": \"\",\n",
    "    \"nfiles\": df[\"nfiles\"].sum(),\n",
    "    \"cpu_hours\": df[\"cpu_hours\"].sum(),\n",
    "    \"Space (GB)\": df[\"Space (GB)\"].sum()\n",
    "}\n",
    "\n",
    "df = pd.concat([df, pd.DataFrame([sum_row])], ignore_index=True)\n",
    "\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets_extra = {\n",
    "  22672: {\"flavor\": \"NuMu\", \"energy\": \"low\", \"true_dataset\": 22672, \"njobs\" : 8000, \"nfiles\" : 7218, \"similar_dataset\" : [23436, 23155] },\n",
    "  22671: {\"flavor\": \"NuMu\", \"energy\": \"mid\", \"true_dataset\": 22671, \"njobs\" : 5000, \"nfiles\" : 4687, \"similar_dataset\" : [23435, 23154]},\n",
    "  22670: {\"flavor\": \"NuMu\", \"energy\": \"high\", \"true_dataset\": 22670, \"njobs\" : 15000, \"nfiles\" : 9688, \"similar_dataset\" : [23434, 23153]},\n",
    "  22668: {\"flavor\": \"NuTau\", \"energy\": \"high\", \"true_dataset\": 22668, \"njobs\" : 20000, \"nfiles\" : 16563, \"similar_dataset\" : [23433, 23152]},\n",
    "  22667: {\"flavor\": \"NuTau\", \"energy\": \"mid\", \"true_dataset\": 22667, \"njobs\" : 4000, \"nfiles\" : 3763, \"similar_dataset\" : [23432, 23151]},\n",
    "  22666: {\"flavor\": \"NuTau\", \"energy\": \"low\", \"true_dataset\": 22666, \"njobs\" : 1000, \"nfiles\" : 989, \"similar_dataset\" : [23431, 23150]},\n",
    "  22665: {\"flavor\": \"NuE\", \"energy\": \"low\", \"true_dataset\": 22665, \"njobs\" : 1000, \"nfiles\" : 989, \"similar_dataset\" : [23430, 23149]},\n",
    "  22664: {\"flavor\": \"NuE\", \"energy\": \"mid\", \"true_dataset\": 22664, \"njobs\" : 4000, \"nfiles\" : 3747, \"similar_dataset\" : [23429, 23148]},\n",
    "  22663: {\"flavor\": \"NuE\", \"energy\": \"high\", \"true_dataset\": 22663, \"njobs\" : 20000, \"nfiles\" : 19693, \"similar_dataset\" : [23428, 23147]}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obtain_cascade_file_size( dataset_id ):\n",
    "\n",
    "    file_paths = [f\"/data/sim/IceCube/2023/filtered/level4/cascade/neutrino-generator/{dataset_id}/0000000-0000999/\",\n",
    "                  f\"/data/sim/IceCube/2023/filtered/level5/cascade/neutrino-generator/cascade/{dataset_id}/0000000-0000999/\",\n",
    "                  f\"/data/sim/IceCube/2023/filtered/level5/cascade/neutrino-generator/hybrid/{dataset_id}/0000000-0000999/\",\n",
    "                  f\"/data/sim/IceCube/2023/filtered/level5/cascade/neutrino-generator/muon/{dataset_id}/0000000-0000999/\",\n",
    "                  f\"/data/sim/IceCube/2023/filtered/level6/cascade/neutrino-generator/cascade/{dataset_id}/0000000-0000999/\",\n",
    "                  f\"/data/sim/IceCube/2023/filtered/level6/cascade/neutrino-generator/hybrid/{dataset_id}/0000000-0000999/\",\n",
    "                  f\"/data/sim/IceCube/2023/filtered/level6/cascade/neutrino-generator/muon/{dataset_id}/0000000-0000999/\"]\n",
    "\n",
    "\n",
    "    result = {\"total\" : 0}\n",
    "\n",
    "    for i,file_path in enumerate(file_paths):\n",
    "\n",
    "        sizes = []\n",
    "        for j,fname in enumerate(os.listdir(file_path)):\n",
    "            sizes.append(os.path.getsize(os.path.join(file_path, fname)))\n",
    "            if j > 20: continue\n",
    "\n",
    "        avg_size = np.mean(sizes) / 1e9  # in GB\n",
    "        result[i] = avg_size\n",
    "\n",
    "        result[\"total\"] +=avg_size\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  dataset        type  nfiles  cpu_hours  Space (GB)\n",
      "0   22672    NuMu_low    7218      36919         836\n",
      "1   22671    NuMu_mid    4687      23852         642\n",
      "2   22670   NuMu_high    9688      29060         519\n",
      "3   22668  NuTau_high   16563      57335        1532\n",
      "4   22667   NuTau_mid    3763      26546         874\n",
      "5   22666   NuTau_low     989      15044         313\n",
      "6   22665     NuE_low     989      28454         611\n",
      "7   22664     NuE_mid    3747      42864        1467\n",
      "8   22663    NuE_high   19693      68336        1832\n",
      "9   Total               67337     328410        8626\n"
     ]
    }
   ],
   "source": [
    "# lets do the extra datasets:\n",
    "rows = []\n",
    "for dataset_id, info in datasets_extra.items():\n",
    "\n",
    "    dataset_id_HESE = info[\"similar_dataset\"][0]\n",
    "    dataset_id_cascade = info[\"similar_dataset\"][1]\n",
    "    dataset_id_true = datasets_tau_reco[dataset_id_HESE][\"true_dataset\"]\n",
    "\n",
    "    # HESE Processing\n",
    "    df = datasets_tau_reco[dataset_id_HESE][\"df\"]\n",
    "    mean_cpu_hours_HESE = extract_mean_usage( \"time_used\",dataset_id_HESE, df )[\"total\"]\n",
    "    average_total_file_size_HESE = obtain_file_size( dataset_id_HESE )[\"total\"]\n",
    "\n",
    "\n",
    "    # cascade processing\n",
    "    df = datasets_level4_6[dataset_id_cascade][\"df\"]\n",
    "    mean_cpu_hours_cascade = extract_mean_usage( \"time_used\",dataset_id_cascade, df )[\"total\"]\n",
    "    average_total_file_size_cascade = obtain_cascade_file_size( dataset_id_true )[\"total\"]\n",
    "    \n",
    "\n",
    "    row = {\n",
    "        \"dataset\": info[\"true_dataset\"],\n",
    "        \"type\": f'{info[\"flavor\"]}_{info[\"energy\"]}',\n",
    "        \"nfiles\": info[\"nfiles\"],\n",
    "        # \"cpu_hours HESE\": int(info[\"nfiles\"]*mean_cpu_hours_HESE),\n",
    "        # \"Space (GB) HESE\": int(info[\"nfiles\"]*average_total_file_size_HESE),\n",
    "        # \"cpu_hours Casc\": int(info[\"nfiles\"]*mean_cpu_hours_cascade),\n",
    "        # \"Space (GB) Casc\": int(info[\"nfiles\"]*average_total_file_size_cascade),\n",
    "        \"cpu_hours\": int(info[\"nfiles\"]*(mean_cpu_hours_cascade+mean_cpu_hours_HESE)),\n",
    "        \"Space (GB)\": int(info[\"nfiles\"]*(average_total_file_size_cascade+average_total_file_size_HESE)), \n",
    "    }\n",
    "    rows.append(row)\n",
    "\n",
    "# Convert list of dicts → DataFrame\n",
    "df = pd.DataFrame(rows)\n",
    "\n",
    "# Add a final row with sums\n",
    "sum_row = {\n",
    "    \"dataset\": \"Total\",\n",
    "    \"type\": \"\",\n",
    "    \"nfiles\": df[\"nfiles\"].sum(),\n",
    "    \"cpu_hours\": df[\"cpu_hours\"].sum(),\n",
    "    \"Space (GB)\": df[\"Space (GB)\"].sum()\n",
    "}\n",
    "\n",
    "df = pd.concat([df, pd.DataFrame([sum_row])], ignore_index=True)\n",
    "\n",
    "print(df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3-v4.4.1_reco-v1.1.0",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
