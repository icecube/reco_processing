{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import matplotlib.pyplot as plt\n",
    "import h5py\n",
    "import pandas as pd\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets_tau_reco = {\n",
    "#   23436: {\"flavor\": \"NuMu\", \"energy\": \"low\", \"true_dataset\": 22646, \"nfiles\" : 8000},\n",
    "  23435: {\"flavor\": \"NuMu\", \"energy\": \"mid\", \"true_dataset\": 22645, \"nfiles\" : 5000},\n",
    "  23434: {\"flavor\": \"NuMu\", \"energy\": \"high\", \"true_dataset\": 22644, \"nfiles\" : 15000},\n",
    "  23433: {\"flavor\": \"NuTau\", \"energy\": \"high\", \"true_dataset\": 22635, \"nfiles\" : 20000},\n",
    "  23432: {\"flavor\": \"NuTau\", \"energy\": \"mid\", \"true_dataset\": 22634, \"nfiles\" : 4000},\n",
    "#   23431: {\"flavor\": \"NuTau\", \"energy\": \"low\", \"true_dataset\": 22633, \"nfiles\" : 1000},\n",
    "#   23430: {\"flavor\": \"NuE\", \"energy\": \"low\", \"true_dataset\": 22614, \"nfiles\" : 1000},\n",
    "  23429: {\"flavor\": \"NuE\", \"energy\": \"mid\", \"true_dataset\": 22613, \"nfiles\" : 4000},\n",
    "  23428: {\"flavor\": \"NuE\", \"energy\": \"high\", \"true_dataset\": 22612, \"nfiles\" : 20000}\n",
    "}\n",
    "\n",
    "datasets_level4_6 = {\n",
    "  23155: {\"flavor\": \"NuMu\", \"energy\": \"low\", \"true_dataset\": 22646},\n",
    "  23154: {\"flavor\": \"NuMu\", \"energy\": \"mid\", \"true_dataset\": 22645},\n",
    "  23153: {\"flavor\": \"NuMu\", \"energy\": \"high\", \"true_dataset\": 22644},\n",
    "  23152: {\"flavor\": \"NuTau\", \"energy\": \"high\", \"true_dataset\": 22635},\n",
    "  23151: {\"flavor\": \"NuTau\", \"energy\": \"mid\", \"true_dataset\": 22634},\n",
    "  23150: {\"flavor\": \"NuTau\", \"energy\": \"low\", \"true_dataset\": 22633},\n",
    "  23149: {\"flavor\": \"NuE\", \"energy\": \"low\", \"true_dataset\": 22614},\n",
    "  23148: {\"flavor\": \"NuE\", \"energy\": \"mid\", \"true_dataset\": 22613},\n",
    "  23147: {\"flavor\": \"NuE\", \"energy\": \"high\", \"true_dataset\": 22612}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open datasets\n",
    "for dataset_id in datasets_tau_reco:\n",
    "    datasets_tau_reco[dataset_id][\"df\"] = df = pd.read_hdf(f\"/data/user/tvaneede/GlobalFit/reco_processing/notebooks/benchmark_tau_reco_iceprod/requirements/data/{dataset_id}.hdf5\", key=f'/{dataset_id}')\n",
    "\n",
    "for dataset_id in datasets_level4_6:\n",
    "    datasets_level4_6[dataset_id][\"df\"] = df = pd.read_hdf(f\"/data/user/tvaneede/GlobalFit/SnowStorm_systematics/iceprod_req_harvest/data/{dataset_id}.hdf5\", key=f'/{dataset_id}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_dict = {\n",
    "    \"low\" : {\n",
    "        0 : \"Level7_Cascade_cascade\",\n",
    "        1 : \"Level8_Cascade_cascade\",\n",
    "    },\n",
    "    \"mid_high\" : {\n",
    "        0 : \"Filter_HESE+Taupede\",\n",
    "        1 : \"EvtGen_HESE\",\n",
    "        2 : \"Level7_Cascade_cascade\",\n",
    "        3 : \"Level8_Cascade_cascade\",\n",
    "    },\n",
    "    \"level4_6\" : {\n",
    "        0 : \"Level4_Cascade\",\n",
    "        1 : \"Level5_Cascade_cascade\",\n",
    "        2 : \"Level5_Cascade_muon\",\n",
    "        3 : \"Level5_Cascade_hybrid\",\n",
    "        4 : \"Level6_Cascade_cascade\",\n",
    "        5 : \"Level6_Cascade_muon\",\n",
    "        6 : \"Level6_Cascade_hybrid\",\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_mean_usage( variable, dataset_id, df, energy ):\n",
    "\n",
    "    # Find the number of tasks in this dataset\n",
    "    tasks = df.index.get_level_values('task').unique()\n",
    "    tasks = tasks[tasks >= 0]  # just in case\n",
    "\n",
    "    result = {\"total\" : 0}\n",
    "\n",
    "    for i, task in enumerate(tasks):\n",
    "        # Mask for this dataset and task\n",
    "        mask = (df.index.get_level_values('dataset') == dataset_id) & \\\n",
    "            (df.index.get_level_values('task') == task)\n",
    "        df_mask = df[mask]\n",
    "        mean = df_mask[variable].mean()\n",
    "\n",
    "        if energy == \"cascade\":\n",
    "            mean = mean\n",
    "        elif energy == \"mid\" or energy == \"high\":\n",
    "            if task == 0 or task == 2:\n",
    "                mean *= 3 # 3 iterations of taupede\n",
    "        else:\n",
    "            if task == 0:\n",
    "                mean *= 3 # 3 iterations of taupede\n",
    "\n",
    "        result[i] = mean; result[\"total\"] += mean\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obtain_file_size( dataset_id ):\n",
    "\n",
    "    true_id = datasets_tau_reco[dataset_id][\"true_dataset\"]\n",
    "\n",
    "    file_paths = [f\"/data/sim/IceCube/2023/filtered/level8/cascade/neutrino-generator/cascade/{true_id}/0000000-0000999/\"]\n",
    "\n",
    "    if \"low\" not in datasets_tau_reco[dataset_id][\"energy\"]:\n",
    "        file_paths += [f\"/data/sim/IceCube/2023/filtered/HESE/neutrino-generator/evtgen/{true_id}/0000000-0000999/\" ]\n",
    "\n",
    "    result = {\"total\" : 0}\n",
    "\n",
    "    for i,file_path in enumerate(file_paths):\n",
    "\n",
    "        sizes = []\n",
    "        for fname in os.listdir(file_path):\n",
    "            sizes.append(os.path.getsize(os.path.join(file_path, fname)))\n",
    "\n",
    "        avg_size = np.mean(sizes) / 1e9  # in GB\n",
    "        result[i] = avg_size\n",
    "\n",
    "        result[\"total\"] +=avg_size\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  dataset        type  nfiles  cpu_hours  Space (GB)\n",
      "0   22645    NuMu_mid    5000      29514         113\n",
      "1   22644   NuMu_high   15000      56434         240\n",
      "2   22635  NuTau_high   20000      81340         518\n",
      "3   22634   NuTau_mid    4000      36264         190\n",
      "4   22613     NuE_mid    4000      66124         342\n",
      "5   22612    NuE_high   20000      86576         506\n",
      "6   Total               68000     356252        1909\n"
     ]
    }
   ],
   "source": [
    "# Collect rows in a list\n",
    "rows = []\n",
    "for dataset_id, info in datasets_tau_reco.items():\n",
    "\n",
    "    df = datasets_tau_reco[dataset_id][\"df\"]\n",
    "\n",
    "    energy = datasets_tau_reco[dataset_id][\"energy\"]\n",
    "\n",
    "    mean_cpu_hours = extract_mean_usage( \"time_used\",dataset_id, df, energy )\n",
    "    average_total_file_size = obtain_file_size( dataset_id )[\"total\"]\n",
    "\n",
    "    row = {\n",
    "        \"dataset\": info[\"true_dataset\"],\n",
    "        \"type\": f'{info[\"flavor\"]}_{info[\"energy\"]}',\n",
    "        \"nfiles\": info[\"nfiles\"],\n",
    "        \"cpu_hours\": int(info[\"nfiles\"]*mean_cpu_hours[\"total\"]), # for bright + deepcore/bright\n",
    "        \"Space (GB)\": int(info[\"nfiles\"]*average_total_file_size), # I will only save evtgen output\n",
    "    }\n",
    "    rows.append(row)\n",
    "\n",
    "# Convert list of dicts → DataFrame\n",
    "df = pd.DataFrame(rows)\n",
    "\n",
    "# Add a final row with sums\n",
    "sum_row = {\n",
    "    \"dataset\": \"Total\",\n",
    "    \"type\": \"\",\n",
    "    \"nfiles\": df[\"nfiles\"].sum(),\n",
    "    \"cpu_hours\": df[\"cpu_hours\"].sum(),\n",
    "    \"Space (GB)\": df[\"Space (GB)\"].sum()\n",
    "}\n",
    "\n",
    "df = pd.concat([df, pd.DataFrame([sum_row])], ignore_index=True)\n",
    "\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets_extra = {\n",
    "  22672: {\"flavor\": \"NuMu\", \"energy\": \"low\", \"true_dataset\": 22672, \"njobs\" : 8000, \"nfiles\" : 7218, \"similar_dataset\" : [23436, 23155] },\n",
    "  22671: {\"flavor\": \"NuMu\", \"energy\": \"mid\", \"true_dataset\": 22671, \"njobs\" : 5000, \"nfiles\" : 4687, \"similar_dataset\" : [23435, 23154]},\n",
    "  22670: {\"flavor\": \"NuMu\", \"energy\": \"high\", \"true_dataset\": 22670, \"njobs\" : 15000, \"nfiles\" : 9688, \"similar_dataset\" : [23434, 23153]},\n",
    "  22668: {\"flavor\": \"NuTau\", \"energy\": \"high\", \"true_dataset\": 22668, \"njobs\" : 20000, \"nfiles\" : 16563, \"similar_dataset\" : [23433, 23152]},\n",
    "  22667: {\"flavor\": \"NuTau\", \"energy\": \"mid\", \"true_dataset\": 22667, \"njobs\" : 4000, \"nfiles\" : 3763, \"similar_dataset\" : [23432, 23151]},\n",
    "  22666: {\"flavor\": \"NuTau\", \"energy\": \"low\", \"true_dataset\": 22666, \"njobs\" : 1000, \"nfiles\" : 989, \"similar_dataset\" : [23431, 23150]},\n",
    "  22665: {\"flavor\": \"NuE\", \"energy\": \"low\", \"true_dataset\": 22665, \"njobs\" : 1000, \"nfiles\" : 989, \"similar_dataset\" : [23430, 23149]},\n",
    "  22664: {\"flavor\": \"NuE\", \"energy\": \"mid\", \"true_dataset\": 22664, \"njobs\" : 4000, \"nfiles\" : 3747, \"similar_dataset\" : [23429, 23148]},\n",
    "  22663: {\"flavor\": \"NuE\", \"energy\": \"high\", \"true_dataset\": 22663, \"njobs\" : 20000, \"nfiles\" : 19693, \"similar_dataset\" : [23428, 23147]}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obtain_cascade_file_size( dataset_id ):\n",
    "\n",
    "    file_paths = [f\"/data/sim/IceCube/2023/filtered/level4/cascade/neutrino-generator/{dataset_id}/0000000-0000999/\",\n",
    "                  f\"/data/sim/IceCube/2023/filtered/level5/cascade/neutrino-generator/cascade/{dataset_id}/0000000-0000999/\",\n",
    "                  f\"/data/sim/IceCube/2023/filtered/level5/cascade/neutrino-generator/hybrid/{dataset_id}/0000000-0000999/\",\n",
    "                  f\"/data/sim/IceCube/2023/filtered/level5/cascade/neutrino-generator/muon/{dataset_id}/0000000-0000999/\",\n",
    "                  f\"/data/sim/IceCube/2023/filtered/level6/cascade/neutrino-generator/cascade/{dataset_id}/0000000-0000999/\",\n",
    "                  f\"/data/sim/IceCube/2023/filtered/level6/cascade/neutrino-generator/hybrid/{dataset_id}/0000000-0000999/\",\n",
    "                  f\"/data/sim/IceCube/2023/filtered/level6/cascade/neutrino-generator/muon/{dataset_id}/0000000-0000999/\"]\n",
    "\n",
    "\n",
    "    result = {\"total\" : 0}\n",
    "\n",
    "    for i,file_path in enumerate(file_paths):\n",
    "\n",
    "        sizes = []\n",
    "        for j,fname in enumerate(os.listdir(file_path)):\n",
    "            sizes.append(os.path.getsize(os.path.join(file_path, fname)))\n",
    "            if j > 20: continue\n",
    "\n",
    "        avg_size = np.mean(sizes) / 1e9  # in GB\n",
    "        result[i] = avg_size\n",
    "\n",
    "        result[\"total\"] +=avg_size\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets_tau_reco = {\n",
    "  23436: {\"flavor\": \"NuMu\", \"energy\": \"low\", \"true_dataset\": 22646, \"nfiles\" : 8000},\n",
    "  23435: {\"flavor\": \"NuMu\", \"energy\": \"mid\", \"true_dataset\": 22645, \"nfiles\" : 5000},\n",
    "  23434: {\"flavor\": \"NuMu\", \"energy\": \"high\", \"true_dataset\": 22644, \"nfiles\" : 15000},\n",
    "  23433: {\"flavor\": \"NuTau\", \"energy\": \"high\", \"true_dataset\": 22635, \"nfiles\" : 20000},\n",
    "  23432: {\"flavor\": \"NuTau\", \"energy\": \"mid\", \"true_dataset\": 22634, \"nfiles\" : 4000},\n",
    "  23431: {\"flavor\": \"NuTau\", \"energy\": \"low\", \"true_dataset\": 22633, \"nfiles\" : 1000},\n",
    "  23430: {\"flavor\": \"NuE\", \"energy\": \"low\", \"true_dataset\": 22614, \"nfiles\" : 1000},\n",
    "  23429: {\"flavor\": \"NuE\", \"energy\": \"mid\", \"true_dataset\": 22613, \"nfiles\" : 4000},\n",
    "  23428: {\"flavor\": \"NuE\", \"energy\": \"high\", \"true_dataset\": 22612, \"nfiles\" : 20000}\n",
    "}\n",
    "\n",
    "# open datasets\n",
    "for dataset_id in datasets_tau_reco:\n",
    "    datasets_tau_reco[dataset_id][\"df\"] = df = pd.read_hdf(f\"/data/user/tvaneede/GlobalFit/reco_processing/notebooks/benchmark_tau_reco_iceprod/requirements/data/{dataset_id}.hdf5\", key=f'/{dataset_id}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  dataset        type  nfiles  cpu_hours  Space (GB)\n",
      "0   22672    NuMu_low    7218      24831         687\n",
      "1   22671    NuMu_mid    4687      38551         544\n",
      "2   22670   NuMu_high    9688      50557         377\n",
      "3   22668  NuTau_high   16563      94238        1140\n",
      "4   22667   NuTau_mid    3763      44626         706\n",
      "5   22666   NuTau_low     989       6558         234\n",
      "6   22665     NuE_low     989      11166         431\n",
      "7   22664     NuE_mid    3747      75498        1162\n",
      "8   22663    NuE_high   19693     117573        1380\n",
      "9   Total               67337     463598        6661\n"
     ]
    }
   ],
   "source": [
    "# lets do the extra datasets:\n",
    "rows = []\n",
    "for dataset_id, info in datasets_extra.items():\n",
    "\n",
    "    dataset_id_HESE = info[\"similar_dataset\"][0]\n",
    "    dataset_id_cascade = info[\"similar_dataset\"][1]\n",
    "    dataset_id_true = datasets_tau_reco[dataset_id_HESE][\"true_dataset\"]\n",
    "    dataset_energy = info[\"energy\"]\n",
    "\n",
    "    # HESE Processing\n",
    "    df = datasets_tau_reco[dataset_id_HESE][\"df\"]\n",
    "    mean_cpu_hours_HESE = extract_mean_usage( \"time_used\",dataset_id_HESE, df, dataset_energy )[\"total\"] \n",
    "    average_total_file_size_HESE = obtain_file_size( dataset_id_HESE )[\"total\"]  \n",
    "\n",
    "    # cascade processing\n",
    "    df = datasets_level4_6[dataset_id_cascade][\"df\"]\n",
    "    mean_cpu_hours_cascade = extract_mean_usage( \"time_used\",dataset_id_cascade, df, \"cascade\" )[\"total\"]\n",
    "    average_total_file_size_cascade = obtain_cascade_file_size( dataset_id_true )[\"total\"]\n",
    "\n",
    "    # total\n",
    "    if \"low\" not in datasets_extra[dataset_id][\"energy\"]:\n",
    "        total_mean_cpu_hours = int(info[\"nfiles\"]* (mean_cpu_hours_HESE + mean_cpu_hours_cascade))\n",
    "        total_average_file_size = int(info[\"nfiles\"]*(average_total_file_size_HESE + average_total_file_size_cascade))\n",
    "    else:\n",
    "        total_mean_cpu_hours = int(info[\"nfiles\"]* (mean_cpu_hours_cascade))\n",
    "        total_average_file_size = int(info[\"nfiles\"]*(average_total_file_size_cascade))\n",
    "\n",
    "    row = {\n",
    "        \"dataset\": info[\"true_dataset\"],\n",
    "        \"type\": f'{info[\"flavor\"]}_{info[\"energy\"]}',\n",
    "        \"nfiles\": info[\"nfiles\"],\n",
    "        # \"cpu_hours HESE\": int(info[\"nfiles\"]*mean_cpu_hours_HESE) if \"low\" not in datasets_extra[dataset_id][\"energy\"] else 0,\n",
    "        # \"Space (GB) HESE\": int(info[\"nfiles\"]*average_total_file_size_HESE) if \"low\" not in datasets_extra[dataset_id][\"energy\"] else 0,\n",
    "        # \"cpu_hours Casc\": int(info[\"nfiles\"]*mean_cpu_hours_cascade),\n",
    "        # \"Space (GB) Casc\": int(info[\"nfiles\"]*average_total_file_size_cascade),\n",
    "        \"cpu_hours\": int(total_mean_cpu_hours),\n",
    "        \"Space (GB)\": int(total_average_file_size), \n",
    "    }\n",
    "    rows.append(row)\n",
    "\n",
    "# Convert list of dicts → DataFrame\n",
    "df = pd.DataFrame(rows)\n",
    "\n",
    "# Add a final row with sums\n",
    "sum_row = {\n",
    "    \"dataset\": \"Total\",\n",
    "    \"type\": \"\",\n",
    "    \"nfiles\": df[\"nfiles\"].sum(),\n",
    "    # \"cpu_hours HESE\": df[\"cpu_hours HESE\"].sum(),\n",
    "    # \"cpu_hours Casc\": df[\"cpu_hours Casc\"].sum(),\n",
    "    \"cpu_hours\": df[\"cpu_hours\"].sum(),\n",
    "    \"Space (GB)\": df[\"Space (GB)\"].sum()\n",
    "}\n",
    "\n",
    "df = pd.concat([df, pd.DataFrame([sum_row])], ignore_index=True)\n",
    "\n",
    "print(df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3-v4.4.1_reco-v1.1.0",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
