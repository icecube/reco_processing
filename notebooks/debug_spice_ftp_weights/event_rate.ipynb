{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import simweights\n",
    "import pickle\n",
    "import os, sys\n",
    "import re\n",
    "import numpy as np\n",
    "import matplotlib as mat\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import matplotlib.colors as colors\n",
    "import matplotlib.gridspec as gridspec\n",
    "import pandas as pd\n",
    "import tables\n",
    "import h5py\n",
    "import math\n",
    "from scipy.stats import mstats\n",
    "import matplotlib as mpl\n",
    "import matplotlib.font_manager as font_manager\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(\"/data/user/tvaneede/GlobalFit/reco_processing/notebooks/weighting\")\n",
    "from weights import *\n",
    "from utils import *\n",
    "from datasets import datasets\n",
    "from selections import selection_mask\n",
    "from fonts import *\n",
    "from plot_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "livetime_yr = 11.687\n",
    "livetime_s  = livetime_yr * 365.25 * 24 * 3600 # 11.687 year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_names = [\"v1_wpid\"]\n",
    "\n",
    "files = {}\n",
    "\n",
    "for dataset_name in dataset_names: files[dataset_name] = datasets[dataset_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "version = \"v1_wpid\"\n",
    "main_plotting_path = f\"/data/user/tvaneede/GlobalFit/reco_processing/notebooks/debug_spice_ftp_weights/output/{version}\"\n",
    "os.system(f\"mkdir -p {main_plotting_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- Extracting files for channel v1_wpid\n"
     ]
    }
   ],
   "source": [
    "# open the files\n",
    "for channel,files_channel in files.items():\n",
    "    print(f\"----- Extracting files for channel {channel}\")\n",
    "    for flavor in files_channel:\n",
    "        files_channel[flavor]['hdf_file'] = pd.HDFStore(files_channel[flavor]['file_path'],'r')\n",
    "        # files_channel[flavor]['variables'] = get_variables_neha( files_channel[flavor]['hdf_file'] )\n",
    "        files_channel[flavor]['weighter'] = simweights.NuGenWeighter( files_channel[flavor]['hdf_file'] ,nfiles=files_channel[flavor]['nfiles'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys_to_merge = {\n",
    "    \"NuE\" : [\"NuE_E2\", \"NuE_E3\"],\n",
    "    \"NuMu\" : [\"NuMu_E2\", \"NuMu_E3\"],\n",
    "    \"NuTau\" : [\"NuTau_E2\", \"NuTau_E3\"],\n",
    "    \"NuAll\" : ['NuE', \"NuMu\", \"NuTau\"],\n",
    "}\n",
    "\n",
    "for channel,files_channel in files.items():\n",
    "    for new_key in keys_to_merge:\n",
    "        files_channel[new_key] = {}\n",
    "        files_channel[new_key]['variables'] = {}\n",
    "        files_channel[new_key]['weighter'] = None\n",
    "\n",
    "        for key in keys_to_merge[new_key]:\n",
    "\n",
    "            # merge the weighters\n",
    "            if files_channel[new_key]['weighter'] == None:\n",
    "                files_channel[new_key]['weighter'] = files_channel[key]['weighter']\n",
    "            else: files_channel[new_key]['weighter'] += files_channel[key]['weighter']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weight functions\n",
    "spline_file = '/data/ana/Diffuse/NNMFit/MCEq_splines/v1.2.1/MCEq_splines_PRI-Gaisser-H4a_INT-SIBYLL23c_allfluxes.pickle'\n",
    "\n",
    "# conventional            \n",
    "flux_keys_conv =  ['conv_antinumu','conv_numu','conv_antinue','conv_nue','conv_antinutau','conv_nutau']\n",
    "spline_object_conv = SplineHandler(spline_file, flux_keys_conv)\n",
    "conv_flux = spline_object_conv.return_weight\n",
    "generator_conv = lambda pdgid, energy, cos_zen: conv_flux(pdgid, energy, cos_zen)\n",
    "\n",
    "# prompt\n",
    "flux_keys_pr =  ['pr_antinumu','pr_numu','pr_antinue','pr_nue','pr_antinutau','pr_nutau']\n",
    "spline_object_pr = SplineHandler(spline_file, flux_keys_pr)\n",
    "pr_flux = spline_object_pr.return_weight\n",
    "generator_pr = lambda pdgid, energy, cos_zen: pr_flux(pdgid, energy, cos_zen)\n",
    "\n",
    "# astro\n",
    "gamma_astro = 2.87\n",
    "per_flavor_norm = 2.12\n",
    "def AstroFluxModel(pdgid, energy, cos_zen):\n",
    "    flux = 0.5*(per_flavor_norm*1e-18)*(energy/1e5)**-gamma_astro\n",
    "    return flux\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "check if merging works now, seems to be fine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "92.58698749516472 14262 2.908033829068569e-06 [0.01304752 0.00587772 0.00184933 ... 0.00140241 0.00121406 0.00176715]\n",
      "14262 1500.400000527501 [13144.37499781  2217.27500528  6659.25000061 ... 13786.97500986\n",
      "  7337.22499423  2148.65000156]\n",
      "14262 False [ True False  True ...  True  True False]\n",
      "5515 2.908033829068569e-06 [0.01304752 0.00184933 0.0123552  ... 0.00167538 0.00140241 0.00121406]\n"
     ]
    }
   ],
   "source": [
    "flavor = \"NuTau_E2\"\n",
    "weights = files_channel[flavor][\"weighter\"].get_weights(AstroFluxModel) * livetime_s\n",
    "print(sum(weights), len(weights),min(weights),weights)\n",
    "HESE_CausalQTot = files_channel[flavor][\"weighter\"].get_column(\"HESE_CausalQTot\", \"value\")\n",
    "print(len(HESE_CausalQTot),min(HESE_CausalQTot),HESE_CausalQTot)\n",
    "mask = HESE_CausalQTot > 6000\n",
    "print(len(mask),min(mask),mask)\n",
    "weights_cut = weights[mask]\n",
    "print(len(weights_cut),min(weights_cut),weights_cut)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          astro_NuTau astro_NuTau_E2 astro_NuTau_E3\n",
      "v1_wpid  17.73 ± 0.40   16.66 ± 0.39    1.06 ± 0.05\n"
     ]
    }
   ],
   "source": [
    "# did the merging go well?\n",
    "data = {}\n",
    "\n",
    "for channel, files_channel in files.items():\n",
    "    channel_data = {}\n",
    "\n",
    "    for flavor in [\"NuTau\", \"NuTau_E2\", \"NuTau_E3\"]:\n",
    "        weights = files_channel[flavor][\"weighter\"].get_weights(AstroFluxModel) * livetime_s\n",
    "        HESE_CausalQTot = files_channel[flavor][\"weighter\"].get_column(\"HESE_CausalQTot\", \"value\")\n",
    "    \n",
    "        ## apply manual HESE_CausalQTot cut\n",
    "        mask = HESE_CausalQTot > 6000\n",
    "        weights_cut = weights[mask]\n",
    "\n",
    "        rate = np.sum(weights_cut)\n",
    "        error = np.sqrt(np.sum(weights_cut**2))\n",
    "        channel_data[f\"astro_{flavor}\"] = f\"{rate:.2f} ± {error:.2f}\"\n",
    "    \n",
    "    data[channel] = channel_data\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame.from_dict(data, orient='index')\n",
    "print(df.to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                               astro_NuE    astro_NuMu   astro_NuTau         conv       prompt\n",
      "neha_track_AllHESE           1.82 ± 0.10  11.80 ± 0.17   2.77 ± 0.11  0.00 ± 0.00  0.06 ± 0.00\n",
      "neha_cascade_AllHESE        53.85 ± 0.55   8.27 ± 0.15  29.18 ± 0.40  0.00 ± 0.00  0.68 ± 0.01\n",
      "neha_doublecascade_AllHESE   1.10 ± 0.07   0.35 ± 0.03   2.95 ± 0.09  0.00 ± 0.00  0.06 ± 0.00\n",
      "total                       56.77 ± 0.56  20.42 ± 0.23  34.90 ± 0.42  0.00 ± 0.00  0.80 ± 0.01\n"
     ]
    }
   ],
   "source": [
    "data = {}\n",
    "\n",
    "for channel, files_channel in files.items():\n",
    "    channel_data = {}\n",
    "\n",
    "    for flavor in ['NuE', \"NuMu\", \"NuTau\"]:\n",
    "        weights = files_channel[flavor][\"weighter\"].get_weights(AstroFluxModel) * livetime_s\n",
    "        rate = np.sum(weights)\n",
    "        error = np.sqrt(np.sum(weights**2))\n",
    "        channel_data[f\"astro_{flavor}\"] = f\"{rate:.2f} ± {error:.2f}\"\n",
    "\n",
    "    # Conventional\n",
    "    weights_conv = files_channel[flavor][\"weighter\"].get_weights(generator_conv) * livetime_s\n",
    "    rate_conv = np.sum(weights_conv)\n",
    "    err_conv = np.sqrt(np.sum(weights_conv**2))\n",
    "    channel_data[\"conv\"] = f\"{rate_conv:.2f} ± {err_conv:.2f}\"\n",
    "\n",
    "    # Prompt\n",
    "    weights_prompt = files_channel[flavor][\"weighter\"].get_weights(generator_pr) * livetime_s\n",
    "    rate_prompt = np.sum(weights_prompt)\n",
    "    err_prompt = np.sqrt(np.sum(weights_prompt**2))\n",
    "    channel_data[\"prompt\"] = f\"{rate_prompt:.2f} ± {err_prompt:.2f}\"\n",
    "\n",
    "    data[channel] = channel_data\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame.from_dict(data, orient='index')\n",
    "\n",
    "# Optional: specify column order\n",
    "columns_order = [f\"astro_{flavor}\" for flavor in ['NuE', 'NuMu', 'NuTau']] + [\"conv\", \"prompt\"]\n",
    "df = df[columns_order]\n",
    "\n",
    "# --- Add total row ---\n",
    "\n",
    "def parse_value_error(s):\n",
    "    \"\"\"Parses 'value ± error' and returns (value, error) as floats.\"\"\"\n",
    "    match = re.match(r\"([0-9.eE+-]+)\\s*±\\s*([0-9.eE+-]+)\", s)\n",
    "    return (float(match[1]), float(match[2])) if match else (0.0, 0.0)\n",
    "\n",
    "channels_to_sum = [\"neha_track_AllHESE\", \"neha_cascade_AllHESE\", \"neha_doublecascade_AllHESE\"]\n",
    "total_row = {}\n",
    "\n",
    "for col in df.columns:\n",
    "    total_val = 0.0\n",
    "    total_err_sq = 0.0\n",
    "    for ch in channels_to_sum:\n",
    "        val, err = parse_value_error(df.loc[ch, col])\n",
    "        total_val += val\n",
    "        total_err_sq += err**2\n",
    "    total_err = np.sqrt(total_err_sq)\n",
    "    total_row[col] = f\"{total_val:.2f} ± {total_err:.2f}\"\n",
    "\n",
    "df.loc['total'] = total_row\n",
    "\n",
    "# Display as string table\n",
    "print(df.to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # reconstructed energy\n",
    "# bins = np.linspace(4.77815,7.1,14)\n",
    "# selection_name = \"RecoETot>60TeV\"\n",
    "# variable = \"RecoETot\"\n",
    "\n",
    "# plotting_path = f\"{main_plotting_path}/{variable}\"\n",
    "# os.system(f\"mkdir -p {plotting_path}\")\n",
    "\n",
    "# for channel,files_channel in files.items():\n",
    "\n",
    "#     fig, ax = plt.subplots(1, 1,figsize =(8,7))\n",
    "\n",
    "#     for flavor in ['NuE', \"NuMu\", \"NuTau\"]:\n",
    "#         selection = selection_mask( files_channel[flavor] )[selection_name]\n",
    "#         y = np.log10(files_channel[flavor]['variables'][variable][ selection ])\n",
    "#         w = files_channel[flavor]['variables']['Weights_Astro'][selection]\n",
    "\n",
    "#         hist,hist_err = make_hist_error(samples=y, bins=bins,weights=w*livetime_s)\n",
    "\n",
    "#         label = f\"{flavor} : {sum(hist):.3f}\"\n",
    "\n",
    "#         plot_hist_errorbar(ax=ax,hist=hist,bins=bins,yerror=hist_err,label=label)\n",
    "    \n",
    "#     # atmospheric neutrinos\n",
    "#     selection = selection_mask( files_channel[\"NuAll\"] ) [selection_name]\n",
    "#     y = np.log10(files_channel['NuAll']['variables'][variable][ selection ])\n",
    "#     w = files_channel['NuAll']['variables']['Weights_Atmospheric'][selection]\n",
    "#     hist,hist_err = make_hist_error(samples=y, bins=bins,weights=w*livetime_s)\n",
    "#     label = f\"Atmospheric : {sum(hist):.3f}\"\n",
    "#     plot_hist_errorbar(ax=ax,hist=hist,bins=bins,yerror=hist_err,label=label)\n",
    "\n",
    "#     ax.legend()\n",
    "#     ax.set_ylabel(f\"Events / {livetime_yr:.2f} yr\",fontdict=font_axis_label)\n",
    "#     ax.set_xlabel(\"Reconstructed deposited energy [log10(GeV)]\",fontdict=font_axis_label)\n",
    "#     ax.set_title(f\"Selected as {channel}\")\n",
    "#     # plt.savefig(f\"{plotting_path}/{variable}_selection-NehaHESE.pdf\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3-v4.4.1_reco-v1.1.0",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
