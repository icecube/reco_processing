{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os, sys\n",
    "import numpy as np\n",
    "import matplotlib as mat\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import matplotlib.colors as colors\n",
    "import matplotlib.gridspec as gridspec\n",
    "import pandas as pd\n",
    "import tables\n",
    "import h5py\n",
    "import math\n",
    "from scipy.stats import mstats\n",
    "import matplotlib as mpl\n",
    "import matplotlib.font_manager as font_manager\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('/data/user/tvaneede/GlobalFit/reco_processing/notebooks/weighting')\n",
    "\n",
    "from datasets import datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check if Neha's Nue, NuMu and NuTau files are the sum of all her files. ALLHESE, no 60 TeV cut!\n",
    "\n",
    "Conclusion: No they are not! I am trusting the separate track, cascade, double cascade files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "flavor_datasets_spice = {\n",
    "    \"NuE\" : [\"22046\", \"22047\", \"22082\", \"22083\"],\n",
    "    \"NuMu\" : [\"22043\", \"22044\", \"22079\", \"22080\"],\n",
    "    \"NuTau\" : [\"22049\", \"22050\", \"22085\", \"22086\"],\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------\n",
      "checking for flavor NuE\n",
      "Combined hdf 45861\n",
      "summed 65680\n",
      "--------------------\n",
      "checking for flavor NuMu\n",
      "Combined hdf 32728\n",
      "summed 46848\n",
      "--------------------\n",
      "checking for flavor NuTau\n",
      "Combined hdf 40816\n",
      "summed 58597\n"
     ]
    }
   ],
   "source": [
    "hdf_path = \"/data/ana/Diffuse/GlobalFit_Flavor/taupede/SnowStorm/RecowithBfr/Baseline/hdf_files/NoDeepCore/AllHESE/\"\n",
    "\n",
    "for flavor in [\"NuE\", \"NuMu\", \"NuTau\"]:\n",
    "\n",
    "    print(20*\"-\")\n",
    "\n",
    "    hdf_total = pd.HDFStore( f\"{hdf_path}/{flavor}.hdf5\",'r')\n",
    "\n",
    "    hdfs = {}\n",
    "\n",
    "    total = 0\n",
    "\n",
    "    print(f\"checking for flavor {flavor}\")\n",
    "    print( \"Combined hdf\", len(hdf_total[\"I3MCWeightDict\"]) )\n",
    "\n",
    "    for filetype in flavor_datasets_spice[flavor]:\n",
    "        for channel in [\"Tracks\", \"Cascades\", \"DoubleCascades\"]:\n",
    "            name = f\"{filetype}_{channel}\"\n",
    "            hdfs[name] = pd.HDFStore( f\"{hdf_path}/{name}.hdf5\",'r')\n",
    "            nevts = len( hdfs[name][\"I3MCWeightDict\"] )\n",
    "            total += nevts\n",
    "            # print( filetype, channel, nevts )\n",
    "\n",
    "    print(\"summed\", total)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check if Neha's 60 TeV cut files could be manually obtained from her ALL files\n",
    "\n",
    "Conclusion: yes that works!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------\n",
      "checking for flavor NuE\n",
      "total_all 65680\n",
      "total_60TeV 61716\n",
      "total_all_cut_60TeV 61716\n",
      "--------------------\n",
      "checking for flavor NuMu\n",
      "total_all 46848\n",
      "total_60TeV 44388\n",
      "total_all_cut_60TeV 44388\n",
      "--------------------\n",
      "checking for flavor NuTau\n",
      "total_all 58597\n",
      "total_60TeV 56249\n",
      "total_all_cut_60TeV 56249\n"
     ]
    }
   ],
   "source": [
    "hdf_path_all = \"/data/ana/Diffuse/GlobalFit_Flavor/taupede/SnowStorm/RecowithBfr/Baseline/hdf_files/NoDeepCore/AllHESE/\"\n",
    "hdf_path_60TeV = \"/data/ana/Diffuse/GlobalFit_Flavor/taupede/SnowStorm/RecowithBfr/Baseline/hdf_files/NoDeepCore/\"\n",
    "\n",
    "for flavor in [\"NuE\", \"NuMu\", \"NuTau\"]:\n",
    "\n",
    "    print(20*\"-\")\n",
    "\n",
    "    hdfs = {}\n",
    "\n",
    "    total_all = 0\n",
    "    total_60TeV = 0\n",
    "    total_all_cut_60TeV = 0\n",
    "\n",
    "    print(f\"checking for flavor {flavor}\")\n",
    "\n",
    "    for filetype in flavor_datasets_spice[flavor]:\n",
    "        for channel in [\"Tracks\", \"Cascades\", \"DoubleCascades\"]:\n",
    "            name = f\"{filetype}_{channel}\"\n",
    "\n",
    "            # all\n",
    "            hdf = pd.HDFStore( f\"{hdf_path_all}/{name}.hdf5\",'r')\n",
    "            nevts = len( hdf[\"I3MCWeightDict\"] )\n",
    "            total_all += nevts\n",
    "\n",
    "            # apply mask\n",
    "            mask = ( hdf[\"RecoETot\"].value > 60e3 )\n",
    "            nevts = len( hdf[\"I3MCWeightDict\"][mask] )\n",
    "            total_all_cut_60TeV += nevts\n",
    "\n",
    "            # 60 TeV files\n",
    "            hdf = pd.HDFStore( f\"{hdf_path_60TeV}/{name}.hdf5\",'r')\n",
    "            nevts = len( hdf[\"I3MCWeightDict\"] )\n",
    "            total_60TeV += nevts\n",
    "\n",
    "    print(\"total_all\", total_all)\n",
    "    print(\"total_60TeV\", total_60TeV)\n",
    "    print(\"total_all_cut_60TeV\", total_all_cut_60TeV)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets check the h5 files that I made from Neha's reco files. See if my sum is the same as the combined datasets. Then also compare with Neha.\n",
    "\n",
    "Conclusion: yes it matches Neha. Now I can use my own hdf5 files that I created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------\n",
      "checking for flavor NuE\n",
      "Combined hdf 65680\n",
      "60 TeV 61716\n",
      "summed 65680\n",
      "--------------------\n",
      "checking for flavor NuMu\n",
      "Combined hdf 46848\n",
      "60 TeV 44388\n",
      "summed 46848\n",
      "--------------------\n",
      "checking for flavor NuTau\n",
      "Combined hdf 58597\n",
      "60 TeV 56249\n",
      "summed 58597\n"
     ]
    }
   ],
   "source": [
    "hdf_path = \"/data/user/tvaneede/GlobalFit/reco_processing/hdf/output/spice_tau_reco/\"\n",
    "\n",
    "for flavor in [\"NuE\", \"NuMu\", \"NuTau\"]:\n",
    "\n",
    "    print(20*\"-\")\n",
    "\n",
    "    hdf_total = pd.HDFStore( f\"{hdf_path}/{flavor}.h5\",'r')\n",
    "\n",
    "    hdfs = {}\n",
    "\n",
    "    total = 0\n",
    "\n",
    "    print(f\"checking for flavor {flavor}\")\n",
    "    print( \"Combined hdf\", len(hdf_total[\"I3MCWeightDict\"]) )\n",
    "\n",
    "    mask = hdf_total[\"RecoETot\"].value > 60e3\n",
    "    print( \"60 TeV\", len(hdf_total[\"I3MCWeightDict\"][mask]) )\n",
    "\n",
    "\n",
    "    for filetype in flavor_datasets_spice[flavor]:\n",
    "        name = f\"{flavor}_{filetype}\"\n",
    "        hdfs[name] = pd.HDFStore( f\"{hdf_path}/{name}.h5\",'r')\n",
    "        nevts = len( hdfs[name][\"I3MCWeightDict\"] )\n",
    "        total += nevts\n",
    "        # print( filetype, channel, nevts )\n",
    "\n",
    "    print(\"summed\", total)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets check the muons, first from Neha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------\n",
      "checking for flavor MuonGun\n",
      "21315 Tracks 19\n",
      "21315 Cascades 0\n",
      "21315 DoubleCascades 0\n",
      "21316 Tracks 1\n",
      "21316 Cascades 0\n",
      "21316 DoubleCascades 0\n",
      "21317 Tracks 0\n",
      "21317 Cascades 0\n",
      "21317 DoubleCascades 0\n",
      "summed 20\n"
     ]
    }
   ],
   "source": [
    "flavor_datasets_spice[\"MuonGun\"] = [\"21315\", \"21316\", \"21317\"]\n",
    "\n",
    "hdf_path = \"/data/ana/Diffuse/GlobalFit_Flavor/taupede/MuonGun/RecowithBfr/hdf_files/NoDeepCore/\"\n",
    "\n",
    "for flavor in [\"MuonGun\"]:\n",
    "\n",
    "    print(20*\"-\")\n",
    "\n",
    "    hdfs = {}\n",
    "\n",
    "    total = 0\n",
    "\n",
    "    print(f\"checking for flavor {flavor}\")\n",
    "\n",
    "    for filetype in flavor_datasets_spice[flavor]:\n",
    "        for channel in [\"Tracks\", \"Cascades\", \"DoubleCascades\"]:\n",
    "            name = f\"{filetype}_{channel}\"\n",
    "            hdfs[name] = pd.HDFStore( f\"{hdf_path}/{name}.hdf5\",'r')\n",
    "            try:\n",
    "                nevts = len( hdfs[name][\"RecoL\"] )\n",
    "            except:\n",
    "                nevts = 0\n",
    "            total += nevts\n",
    "            print( filetype, channel, nevts )\n",
    "\n",
    "    print(\"summed\", total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets check my Muon Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------\n",
      "checking for flavor MuonGun\n",
      "21315 0\n",
      "21316 0\n",
      "21317 0\n",
      "summed 0\n"
     ]
    }
   ],
   "source": [
    "hdf_path = \"/data/user/tvaneede/GlobalFit/reco_processing/hdf/output/spice_tau_reco\"\n",
    "\n",
    "for flavor in [\"MuonGun\"]:\n",
    "\n",
    "    print(20*\"-\")\n",
    "\n",
    "    hdfs = {}\n",
    "\n",
    "    total = 0\n",
    "\n",
    "    print(f\"checking for flavor {flavor}\")\n",
    "\n",
    "    for filetype in flavor_datasets_spice[flavor]:\n",
    "        name = f\"{flavor}_{filetype}\"\n",
    "        hdfs[name] = pd.HDFStore( f\"{hdf_path}/{name}.h5\",'r')\n",
    "        try:\n",
    "            nevts = len( hdfs[name][\"RecoL\"] )\n",
    "        except:\n",
    "            nevts = 0\n",
    "        total += nevts\n",
    "        print( filetype, nevts )\n",
    "\n",
    "    print(\"summed\", total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets make a table of the number of Neha's events for each flavor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flavor |       HESE |  RecoETot > 60 TeV\n",
      "----------------------------------------\n",
      "NuE    |      65680 |              61716\n",
      "NuMu   |      46848 |              44388\n",
      "NuTau  |      58597 |              56249\n"
     ]
    }
   ],
   "source": [
    "hdf_path = \"/data/user/tvaneede/GlobalFit/reco_processing/hdf/output/spice_tau_reco/\"\n",
    "\n",
    "# Table header\n",
    "print(f\"{'Flavor':<6} | {'HESE':>10} | {'RecoETot > 60 TeV':>18}\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "for flavor in [\"NuE\", \"NuMu\", \"NuTau\"]:\n",
    "    hdf_total = pd.HDFStore(f\"{hdf_path}/{flavor}.h5\", 'r')\n",
    "    \n",
    "    total_count = len(hdf_total[\"I3MCWeightDict\"])\n",
    "    mask = hdf_total[\"RecoETot\"].value > 60e3\n",
    "    masked_count = len(hdf_total[\"I3MCWeightDict\"][mask])\n",
    "    \n",
    "    print(f\"{flavor:<6} | {total_count:>10} | {masked_count:>18}\")\n",
    "    \n",
    "    hdf_total.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets do the snowstorm simulations for Neha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "flavor_datasets_spice_ensemble = {\n",
    "    \"NuE\" : [\"22014\", \"22015\"],\n",
    "    \"NuMu\" : [\"22011\", \"22012\"],\n",
    "    \"NuTau\" : [\"22017\", \"22018\"],   \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flavor |       HESE |  RecoETot > 60 TeV\n",
      "----------------------------------------\n",
      "NuE    |     131712 |             123581\n",
      "NuMu   |      93963 |              89092\n",
      "NuTau  |     116876 |             112284\n"
     ]
    }
   ],
   "source": [
    "hdf_path = \"/data/user/tvaneede/GlobalFit/reco_processing/hdf/output/spice_tau_reco_ensemble/\"\n",
    "\n",
    "# Table header\n",
    "print(f\"{'Flavor':<6} | {'HESE':>10} | {'RecoETot > 60 TeV':>18}\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "for flavor in [\"NuE\", \"NuMu\", \"NuTau\"]:\n",
    "    hdf_total = pd.HDFStore(f\"{hdf_path}/{flavor}.h5\", 'r')\n",
    "    \n",
    "    total_count = len(hdf_total[\"I3MCWeightDict\"])\n",
    "    mask = hdf_total[\"RecoETot\"].value > 60e3\n",
    "    masked_count = len(hdf_total[\"I3MCWeightDict\"][mask])\n",
    "    \n",
    "    print(f\"{flavor:<6} | {total_count:>10} | {masked_count:>18}\")\n",
    "    \n",
    "    hdf_total.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets do the ftp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "flavor_datasets_ftp = {\n",
    "    \"NuE\" : [\"22612\", \"22613\", \"22663\", \"22664\"],\n",
    "    \"NuMu\" : [\"22644\", \"22645\", \"22670\", \"22671\"],\n",
    "    \"NuTau\" : [\"22634\", \"22635\", \"22667\", \"22666\"],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flavor |      Total |               HESE\n",
      "----------------------------------------\n",
      "NuE    |    2125204 |             244095\n",
      "NuMu   |    1961223 |             112840\n",
      "NuTau  |    2009543 |             212300\n"
     ]
    }
   ],
   "source": [
    "hdf_path = \"/data/user/tvaneede/GlobalFit/reco_processing/hdf/output/ftp_l3casc/\"\n",
    "\n",
    "# Table header\n",
    "print(f\"{'Flavor':<6} | {'Total':>10} | {'HESE':>18}\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "for flavor in [\"NuE\", \"NuMu\", \"NuTau\"]:\n",
    "    hdf_total = pd.HDFStore(f\"{hdf_path}/{flavor}.h5\", 'r')\n",
    "    \n",
    "    total_count = len(hdf_total[\"I3MCWeightDict\"])\n",
    "    mask = hdf_total[\"HESE_CausalQTot\"].value > 6000\n",
    "    masked_count = len(hdf_total[\"HESE_CausalQTot\"][mask])\n",
    "\n",
    "    print(f\"{flavor:<6} | {total_count:>10} | {masked_count:>18}\")\n",
    "    \n",
    "    hdf_total.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flavor |      Total |               HESE\n",
      "----------------------------------------\n",
      "NuE    |    1281899 |             154383\n",
      "NuMu   |    1192064 |              71187\n",
      "NuTau  |    1115825 |             120820\n"
     ]
    }
   ],
   "source": [
    "hdf_path = \"/data/user/tvaneede/GlobalFit/reco_processing/hdf/output/ftp_l3casc_ensemble/\"\n",
    "\n",
    "# Table header\n",
    "print(f\"{'Flavor':<6} | {'Total':>10} | {'HESE':>18}\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "for flavor in [\"NuE\", \"NuMu\", \"NuTau\"]:\n",
    "    hdf_total = pd.HDFStore(f\"{hdf_path}/{flavor}.h5\", 'r')\n",
    "    \n",
    "    total_count = len(hdf_total[\"I3MCWeightDict\"])\n",
    "    mask = hdf_total[\"HESE_CausalQTot\"].value > 6000\n",
    "    masked_count = len(hdf_total[\"HESE_CausalQTot\"][mask])\n",
    "    \n",
    "    print(f\"{flavor:<6} | {total_count:>10} | {masked_count:>18}\")\n",
    "    \n",
    "    hdf_total.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3-v4.4.1_reco-v1.1.0",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
